{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi  # check GPU usage -- can ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this cell -- stuff for our server\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['HTTP_PROXY']='http://proxy:3128/'\n",
    "os.environ['HTTPS_PROXY']='http://proxy:3128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a28afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimage\n",
    "\n",
    "from data.utils import parse_image_example\n",
    "from modeling.layers import ConvNormAct, ResidualBlock\n",
    "\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = np.pad(train_images[..., None], ((0, 0), (2, 2), (2, 2), (0, 0))).astype(np.float32) / 255.\n",
    "test_images = np.pad(test_images[..., None], ((0, 0), (2, 2), (2, 2), (0, 0))).astype(np.float32) / 255.\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size, drop_remainder=True)\n",
    "test_data = tf.data.Dataset.from_tensor_slices(test_images).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89044e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.concatenate([batch for batch in iter(test_data)], axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for ind, img in enumerate(test_images[:64]):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.imshow(img, vmin=0, vmax=1, cmap=\"Greys\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(train_images.reshape(-1), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25552222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the improved paper version, we need the maximum euclidean distance between data points\n",
    "# this runs basically forever; just cancelling it after some time is fine though\n",
    "train_images_flat = test_images.reshape((-1, 32*32))\n",
    "max_distance_so_far = 0.\n",
    "\n",
    "_batchs = 64\n",
    "for ind in range(0, len(train_images_flat), _batchs):\n",
    "    img = train_images_flat[ind:ind+_batchs]\n",
    "    all_distances = np.sqrt(np.sum((train_images_flat[None] - img[:, None])**2, axis=-1))\n",
    "    max_distance_here = all_distances.max()\n",
    "    max_distance_so_far = np.maximum(max_distance_here, max_distance_so_far)\n",
    "    if not ind % 10:\n",
    "        print(ind, max_distance_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many scales??\n",
    "from scipy import stats\n",
    "\n",
    "d = 32*32  # data dimensionality\n",
    "wish_gamma = 1.05  # <-- this is the target ratio between successive noise scales, try values > 1\n",
    "\n",
    "upper_limit = np.sqrt(2*d) * (wish_gamma - 1) + 3*wish_gamma\n",
    "lower_limit = np.sqrt(2*d) * (wish_gamma - 1) - 3*wish_gamma\n",
    "\n",
    "c_value = stats.norm.cdf(upper_limit) - stats.norm.cdf(lower_limit)\n",
    "\n",
    "print(\"C value is {}; should be 0.5 or higher! Too low? Make gamma smaller!\".format(c_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9424d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_noise_scales = 200\n",
    "target_noise = 0.001\n",
    "noise_scales = np.geomspace(max_distance_so_far, target_noise, n_noise_scales, dtype=np.float32)\n",
    "true_gamma = noise_scales[0] / noise_scales[1]\n",
    "print(\"Gamma is {}, should be {} or lower! Too high? Make n_noise_scales larger!\".format(true_gamma, wish_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13798175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for langevin sampler\n",
    "def wow_formula(gamma, t, eps):\n",
    "    final_sig_sq = noise_scales[-1]**2\n",
    "    first = (1 - (eps / final_sig_sq))**(2*t)\n",
    "    second = gamma**2 - 2*eps / (final_sig_sq - final_sig_sq * (1 - eps/final_sig_sq)**2)\n",
    "    third = 2*eps / (final_sig_sq - final_sig_sq * (1 - eps/final_sig_sq)**2)\n",
    "    \n",
    "    return first*second + third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d714c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = 2000\n",
    "t_per_noise_scale = t_total // n_noise_scales\n",
    "\n",
    "epsilon = 0.00000007\n",
    "some_value = wow_formula(true_gamma, t_per_noise_scale, epsilon)\n",
    "\n",
    "print(\"The thingy value is {}! It should be close to 1! Try playing around with the epsilon value.\".format(some_value))\n",
    "print(t_per_noise_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbb9e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for scale in noise_scales[::20]:\n",
    "    noisy_imgs = train_images[:64] + scale*np.random.normal(size=(64, 32, 32, 1))\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for ind, image in enumerate(noisy_imgs):\n",
    "        plt.subplot(8, 8, ind+1)\n",
    "        plt.imshow(image, cmap=\"Greys\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.suptitle(\"Noise scale: {}\".format(scale))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(noisy_imgs.reshape(-1), bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa406cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreMatching(tf.keras.Model):\n",
    "    def __init__(self, inputs, outputs, noise_scales, **kwargs):\n",
    "        super().__init__(inputs, outputs, **kwargs)\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(\"loss\")\n",
    "        \n",
    "        self.num_noise_scales = len(noise_scales)\n",
    "        self.noise_scales_tensor = tf.convert_to_tensor(noise_scales, dtype=tf.float32)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.denoising_score_matching_loss(data, training=True)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        loss = self.denoising_score_matching_loss(data, training=False)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "    \n",
    "    def denoising_score_matching_loss(self, image_batch, training=None):\n",
    "        sampled_noise_index = tf.random.uniform([1], 0, self.num_noise_scales, dtype=tf.int32)[0]\n",
    "        noise = self.noise_scales_tensor[sampled_noise_index]\n",
    "\n",
    "        noisy_batch = image_batch + noise * tf.random.normal(tf.shape(image_batch))\n",
    "        noise_input = tf.repeat(noise, tf.shape(noisy_batch)[0])[:, None, None, None]\n",
    "\n",
    "        score = self([noisy_batch, noise_input], training=training)\n",
    "        target_score = -1 * (noisy_batch - image_batch) / noise\n",
    "        loss = tf.reduce_mean(0.5 * tf.reduce_sum((score - target_score)**2, axis=[1,2,3]))\n",
    "\n",
    "        weight = 1\n",
    "\n",
    "        return weight * loss\n",
    "    \n",
    "    # this is not used\n",
    "    def FULLdenoising_score_matching_loss(self, image_batch, training=None):\n",
    "        total_loss = 0.\n",
    "        for noise in self.noise_scales_tensor:\n",
    "            noisy_batch = image_batch + noise * tf.random.normal(tf.shape(image_batch))\n",
    "            noise_input = tf.repeat(noise, tf.shape(noisy_batch)[0])[:, None, None, None]\n",
    "\n",
    "            score = self([noisy_batch, noise_input], training=training)\n",
    "            target_score = -1 * (noisy_batch - image_batch) / noise\n",
    "            loss = tf.reduce_mean(0.5 * tf.reduce_sum((score - target_score)**2, axis=[1,2,3]))\n",
    "\n",
    "            weight = 1 / self.num_noise_scales\n",
    "            total_loss += weight * loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    @tf.function(jit_compile=True)\n",
    "    def langevin_step(self, sample, alpha, noise, noise_input):\n",
    "        sample = (sample \n",
    "                  + alpha*self([sample, noise_input], training=False) / noise\n",
    "                  + tf.math.sqrt(2*alpha)*tf.random.normal(tf.shape(sample)))\n",
    "        return sample\n",
    "\n",
    "    def langevin_sampler(self, n_steps, epsilon, n_samples=64, denoise=True, show_intermediate=False):\n",
    "        sample = self.noise_scales_tensor[0] * tf.random.normal((n_samples,) + self.input_shape[0][1:])\n",
    "\n",
    "        if show_intermediate:\n",
    "            plt.figure(figsize=(15, 15))\n",
    "            for ind, image in enumerate(sample):\n",
    "                plt.subplot(8, 8, ind+1)\n",
    "                plt.imshow(image, cmap=\"Greys\")\n",
    "                plt.axis(\"off\")\n",
    "            plt.suptitle(\"Initial samples\")\n",
    "            plt.show()\n",
    "\n",
    "        for index, noise in enumerate(self.noise_scales_tensor):\n",
    "            alpha = tf.cast(epsilon * (noise / self.noise_scales_tensor[-1])**2, tf.float32)\n",
    "            noise_input = tf.repeat(noise, n_samples, axis=0)[:, None, None, None]\n",
    "\n",
    "            for step in tf.range(n_steps):\n",
    "                sample = self.langevin_step(sample, alpha, noise, noise_input)\n",
    "\n",
    "            if show_intermediate and not index % show_intermediate:\n",
    "                plt.figure(figsize=(15, 15))\n",
    "                for ind, image in enumerate(sample):\n",
    "                    plt.subplot(8, 8, ind+1)\n",
    "                    plt.imshow(image, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "                    plt.axis(\"off\")\n",
    "                plt.suptitle(\"Noise scale {}\".format(noise))\n",
    "                plt.show()\n",
    "                \n",
    "        if denoise:\n",
    "            sample = sample + noise**2 * score_model([sample, noise_input]) / noise\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "norm = lambda **kwargs: tfa.layers.GroupNormalization(groups=8, **kwargs)\n",
    "\n",
    "def residual_stack(inputs, filters, strides, blocks_per_level, mode, name):\n",
    "    all_outputs = []\n",
    "    outputs = inputs\n",
    "    for level_ind, (level_filters, level_stride) in enumerate(zip(filters, strides)):\n",
    "        for block_ind in range(blocks_per_level):\n",
    "            outputs = ResidualBlock(level_filters,\n",
    "                                    mode, \n",
    "                                    strides=level_stride if block_ind == 0 else 1,\n",
    "                                    name=\"_\".join([name, str(level_ind+1), str(block_ind+1)]),\n",
    "                                    normalization=norm)(outputs)\n",
    "            all_outputs.append(outputs)\n",
    "        \n",
    "    return outputs, all_outputs\n",
    "\n",
    "def residual_stack_d(inputs, all_hidden, filters, strides, blocks_per_level, mode, name):\n",
    "    outputs = inputs\n",
    "    global_ind = 0\n",
    "    for level_ind, (level_filters, level_stride) in enumerate(zip(filters, strides)):\n",
    "        for block_ind in range(blocks_per_level):\n",
    "            if global_ind > 0:\n",
    "                if outputs.shape[1] != all_hidden[global_ind].shape[1]:\n",
    "                    all_hidden[global_ind] = tfkl.AvgPool2D(padding=\"same\")(all_hidden[global_ind])\n",
    "                outputs = tf.concat((outputs, all_hidden[global_ind]), axis=-1)\n",
    "            global_ind += 1\n",
    "            \n",
    "            outputs = ResidualBlock(level_filters,\n",
    "                                    mode, \n",
    "                                    strides=level_stride if block_ind == 0 else 1,\n",
    "                                    name=\"_\".join([name, str(level_ind+1), str(block_ind+1)]),\n",
    "                                    normalization=norm)(outputs)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.Input((32, 32, 1))\n",
    "noise_input = tf.keras.Input((1, 1, 1))\n",
    "\n",
    "blocks_per_level = 2\n",
    "filters = [32, 64, 128, 256]\n",
    "strides = [1, 2, 2, 2]\n",
    "encoder_output, all_hidden = residual_stack(inp, filters, strides, blocks_per_level, \"conv\", \"encoder\")\n",
    "\n",
    "decoder_output = residual_stack_d(encoder_output, list(reversed(all_hidden)), reversed(filters), strides, blocks_per_level, \"upconv\", \"decoder\")\n",
    "decoder_final = tfkl.Conv2D(1, 1)(decoder_output)\n",
    "\n",
    "score_model = ScoreMatching([inp, noise_input], decoder_final, noise_scales)\n",
    "score_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc693f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema = 0.9998\n",
    "t = np.arange(100000)\n",
    "plt.plot(t, ema**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20747692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 100000\n",
    "n_data = 60000\n",
    "n_epochs = train_steps // (n_data // batch_size)\n",
    "lr = tf.optimizers.schedules.CosineDecay(0.001, train_steps)\n",
    "optimizer = tf.optimizers.Adam(lr, use_ema=True, ema_momentum=ema)\n",
    "\n",
    "score_model.compile(optimizer=optimizer, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dac100",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ImageGenCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, frequency, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not epoch % self.frequency:\n",
    "            generated_batch = self.model.langevin_sampler(t_per_noise_scale, epsilon)\n",
    "        \n",
    "            plt.figure(figsize=(15,15))\n",
    "            for ind, image in enumerate(generated_batch):\n",
    "                plt.subplot(8, 8, ind+1)\n",
    "                plt.imshow(image, vmin=0, vmax=1, cmap=\"Greys\")\n",
    "                plt.axis(\"off\")\n",
    "            plt.suptitle(\"Random generations\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "score_model.fit(train_data, validation_data=test_data, epochs=n_epochs, callbacks=ImageGenCallback(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb691610",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model.save_weights(\"weights/weights_score_mnist.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2166e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = score_model.langevin_sampler(t_per_noise_scale, epsilon, n_samples=256)\n",
    "plt.figure(figsize=(15,15))\n",
    "for ind, image in enumerate(gens):\n",
    "    plt.subplot(16, 16, ind+1)\n",
    "    plt.imshow(image, vmin=0, vmax=1, cmap=\"Greys\")\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Random generations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other things like \"partial generation\" or inpainint can also be done.\n",
    "# see the _large notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
