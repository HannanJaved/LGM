{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this cell when running on colab\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one too!\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1\n",
    "\n",
    "# define random stochatic matrix\n",
    "# NOTE: in principle it would be interesting how stuff behaves\n",
    "# as n becomes larger. but then the random matrix will become\n",
    "# more and more uniform, making it quite boring :(\n",
    "# so the number of states is kept rather small.\n",
    "n_states = 20\n",
    "n_steps = 10\n",
    "log_transition_matrix = tf.random.normal([n_states, n_states])\n",
    "transition_matrix = tf.nn.softmax(log_transition_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial distribution v0, repeatedly multiply with A\n",
    "# -> should always result in approximately the same v after\n",
    "#    a few iterations\n",
    "# bonus: compute eigenvectors/values of A; compare to v\n",
    "\n",
    "v_start = tf.random.normal([n_states, 1])\n",
    "v_start = tf.nn.softmax(v_start, axis=0)\n",
    "\n",
    "v_new = v_start\n",
    "diffs = []\n",
    "for st in range(n_steps):\n",
    "    v_old = v_new\n",
    "    v_new = tf.matmul(transition_matrix, v_new)\n",
    "    diffs.append(((v_new - v_old)**2).numpy().sum())\n",
    "    \n",
    "# we can see that v converges very quickly\n",
    "plt.plot(np.arange(1, len(diffs)+1), diffs)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Change in v\")\n",
    "plt.show()\n",
    "\n",
    "print(\"v0\")\n",
    "print(v_start)\n",
    "\n",
    "print(\"\\n A\")\n",
    "print(transition_matrix)\n",
    "\n",
    "print(\"\\n v'\")\n",
    "print(v_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unless np.eig behaves quirkily, this should be about the same\n",
    "eigenvector = np.linalg.eig(transition_matrix)[1][:, 0]\n",
    "print((eigenvector / eigenvector.sum()).real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we run a Markov chain!\n",
    "# x = 0 as initial state is arbitrary\n",
    "# could be any state, or sample it randomly\n",
    "\n",
    "x = 0\n",
    "\n",
    "all_samples = []\n",
    "n_steps = 10000\n",
    "for st in range(n_steps):\n",
    "    # the indexing here is a bit annoying. basically we just grab the column\n",
    "    # that corresponds to the current state\n",
    "    x = tf.random.categorical(log_transition_matrix[tf.newaxis, :, x], 1)[0, 0]\n",
    "    #print(x)\n",
    "    all_samples.append(x.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue bars: empirical distribution\n",
    "# orange stars: distribution via A*v\n",
    "plt.hist(all_samples, bins=np.linspace(-0.5, n_states-0.5, n_states+1), rwidth=0.9)\n",
    "plt.plot(v_new.numpy()*n_steps, \"*\")\n",
    "plt.xlabel(\"State index\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# loc is the most important parameter! rerun this multiple times with different values,\n",
    "# e.g. start with 0.1, try 1., 3., 5...\n",
    "# FYI tfp might crash if you provide an integer, so always make it a float...\n",
    "\n",
    "location = 3\n",
    "scale = 1.\n",
    "sample = np.array([0.,0.])  # initial sample -- arbitrary\n",
    "# \"balanced\" Gaussian mixture\n",
    "basic_distribution = tfd.MixtureSameFamily(tfd.Categorical(probs=[0.5, 0.5]), \n",
    "                                   tfd.MultivariateNormalDiag(loc=[[-location, -location], [location, location]],\n",
    "                                                             scale_identity_multiplier=scale))\n",
    "\n",
    "n_steps = 1000\n",
    "samples = [sample]\n",
    "for step in range(n_steps):\n",
    "    sample = sample.copy()\n",
    "    # sample new x1 (given OLD x2)\n",
    "    mixing_score1 = tfd.Normal(loc=-location, scale=scale).prob(sample[1])\n",
    "    mixing_score2 = tfd.Normal(loc=location, scale=scale).prob(sample[1])\n",
    "    pi1 = mixing_score1 / (mixing_score1 + mixing_score2)\n",
    "    pi2 = mixing_score2 / (mixing_score1 + mixing_score2)\n",
    "    # the normal distributions stay the same (same mean)\n",
    "    # but the mixture coefficients are different.\n",
    "    # note that we are now using a 1D normal because we are only sampling one variable\n",
    "    conditional_distribution = tfd.MixtureSameFamily(tfd.Categorical(probs=[pi1, pi2]),\n",
    "                                      tfd.Normal(loc=[-location, location],\n",
    "                                                 scale=scale))\n",
    "    sample[0] = conditional_distribution.sample().numpy()\n",
    "    \n",
    "    # sample new x2 given the NEW x1!!\n",
    "    mixing_score1 = tfd.Normal(loc=-location, scale=scale).prob(sample[0])\n",
    "    mixing_score2 = tfd.Normal(loc=location, scale=scale).prob(sample[0])\n",
    "    pi1 = mixing_score1 / (mixing_score1 + mixing_score2)\n",
    "    pi2 = mixing_score2 / (mixing_score1 + mixing_score2)\n",
    "    conditional_distribution = tfd.MixtureSameFamily(tfd.Categorical(probs=[pi1, pi2]),\n",
    "                                      tfd.Normal(loc=[-location, location],\n",
    "                                                 scale=scale))\n",
    "    sample[1] = conditional_distribution.sample().numpy()\n",
    "    samples.append(sample)\n",
    "samples = np.array(samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 2*location if location >= 2 else 4\n",
    "grid_points = np.linspace(-lim, lim, 1000)\n",
    "grid_x, grid_y = np.meshgrid(grid_points, grid_points)\n",
    "grid = np.stack([grid_x, grid_y], axis=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour plot shows the real distribution\n",
    "# black dots show samples\n",
    "# lines show the steps between samples\n",
    "# the earliest samples have lines in orange and over the course of sampling,\n",
    "# the line becomes more blue\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.contour(grid_x, grid_y, basic_distribution.prob(grid))\n",
    "\n",
    "color_gradient = np.linspace(0,1,len(samples))\n",
    "for i in range(len(samples)-1):\n",
    "    plt.plot(samples[i:i+2, 0],samples[i:i+2, 1], color=(1-color_gradient[i],\n",
    "                                                         0.5,\n",
    "                                                         color_gradient[i]), alpha=0.3)\n",
    "\n",
    "#plt.plot(samples[:,0], samples[:,1], c=\"red\", alpha=0.3, lw=0.6)\n",
    "plt.plot(samples[:,0], samples[:,1], \".\", c=\"black\", alpha=0.5)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map gives a better view of whether both modes have been sampled evenly\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist2d(samples[:,0], samples[:,1], bins=30)\n",
    "plt.show()\n",
    "\n",
    "# bonus: this could be done more properly via statistical testing.\n",
    "# i.e. check whether the amount of samples per quadrant is significantly\n",
    "# different from what we would expect (should be about equal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple count\n",
    "quadrant1 = np.logical_and(samples[:, 0] >= 0, samples[:, 1] >= 0).sum()\n",
    "#quadrant2 = np.logical_and(samples[:, 0] >= 0, samples[:, 1] < 0).sum()\n",
    "#quadrant3 = np.logical_and(samples[:, 0] < 0, samples[:, 1] >= 0).sum()\n",
    "quadrant4 = np.logical_and(samples[:, 0] < 0, samples[:, 1] < 0).sum()\n",
    "\n",
    "print(quadrant1, quadrant4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Part 3 -- Importance Sampling\n",
    "# we sample from q, but try to estimate the average norm for samples from p\n",
    "# (here called basic_dist).\n",
    "\n",
    "# for this we use the \"correction factor\" in importance sampling.\n",
    "\n",
    "# try this for different locations again and note how the estimate\n",
    "# breaks down as p moves away from q.\n",
    "\n",
    "# warning: can take a while to run!\n",
    "\n",
    "def try_importance_sampling(n_samples, location, scale=1.):\n",
    "    q_distribution = tfd.MultivariateNormalDiag(loc=[0.,0.], scale_identity_multiplier=1.)\n",
    "    basic_distribution = tfd.MixtureSameFamily(tfd.Categorical(probs=[0.5, 0.5]), \n",
    "                                       tfd.MultivariateNormalDiag(loc=[[-location, -location], [location, location]],\n",
    "                                                                 scale_identity_multiplier=scale))\n",
    "\n",
    "    true_norms = []\n",
    "    fake_norms = []\n",
    "    for step in range(n_samples):\n",
    "        if not step % 500:\n",
    "            print(\"Went through\", step)\n",
    "        true_sample = basic_distribution.sample()\n",
    "        true_norms.append(tf.sqrt(tf.reduce_sum(true_sample**2)).numpy())\n",
    "\n",
    "        fake_sample = q_distribution.sample()\n",
    "        f = tf.sqrt(tf.reduce_sum(fake_sample**2)) * basic_distribution.prob(fake_sample) / q_distribution.prob(fake_sample)\n",
    "        fake_norms.append(f.numpy())\n",
    "\n",
    "    cumulative_norm_sum = np.cumsum(true_norms)\n",
    "    cumulative_norm_sum /= np.arange(1,len(cumulative_norm_sum)+1)\n",
    "\n",
    "    cumulative_norm_sum_fake = np.cumsum(fake_norms)\n",
    "    cumulative_norm_sum_fake /= np.arange(1, len(cumulative_norm_sum_fake)+1)\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(cumulative_norm_sum, label=\"Real samples\")\n",
    "    plt.plot(cumulative_norm_sum_fake, label=\"Importance sampling\")\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.ylabel(\"Estimate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_importance_sampling(5000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_importance_sampling(5000, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_importance_sampling(5000, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_importance_sampling(5000, 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_importance_sampling(5000, 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ASSIGNMENT 3\n",
    "# simple way to get samples from a bernoulli distribution\n",
    "distribution = tfd.Bernoulli(probs=0.5)\n",
    "distribution.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also give dimensions to sample, to get more samples\n",
    "batch_size = 4\n",
    "n_v = 3\n",
    "\n",
    "# here, each sample is 1 with probability 0.5\n",
    "distribution.sample((batch_size, n_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also create a \"multi-dimensional distribution\"\n",
    "# with different probabilities per dimension\n",
    "\n",
    "# we could pretend that these are RBM outputs\n",
    "probs_v_given_h = tf.nn.sigmoid(tf.random.normal((batch_size, n_v)))\n",
    "\n",
    "# same shape as above, however each sample is 1 with a different probability!\n",
    "distribution = tfd.Bernoulli(probs=probs_v_given_h)\n",
    "distribution.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the corresponding probabilities\n",
    "probs_v_given_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
