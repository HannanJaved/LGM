{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi  # check GPU usage -- can ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT run this cell -- stuff for our server\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['HTTP_PROXY']='http://proxy:3128/'\n",
    "os.environ['HTTPS_PROXY']='http://proxy:3128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimage\n",
    "\n",
    "from data.utils import parse_image_example\n",
    "from modeling.layers import ConvNormAct, ResidualBlock\n",
    "\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "# this assumes a preprocessed flickr-faces dataset (code is also in the repository)\n",
    "# you can replace this with MNIST or CIFAR or whatever you'd like :)\n",
    "\n",
    "batch_size = 128\n",
    "# I use the 32x32 version here\n",
    "train_data = tf.data.TFRecordDataset(\"data/flickr_32_train.TFR\").shuffle(60000).map(parse_image_example).batch(batch_size)\n",
    "test_data = tf.data.TFRecordDataset(\"data/flickr_32_test.TFR\").map(parse_image_example).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some images to confirm they look good\n",
    "test_images = np.concatenate([batch for batch in iter(test_data)], axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for ind, img in enumerate(test_images[:64]):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few options for loss functions\n",
    "# all of these SUM over the image dimensions (height, width, channels)\n",
    "# -- recall the discussion in the exercise\n",
    "\n",
    "def squared_loss(y_true, y_pred):\n",
    "    # this is what we get if assuming a gaussian likelihood and unknown fixed sigma\n",
    "    batch_shape = tf.shape(y_true)[0]\n",
    "    y_true = tf.reshape(y_true, [batch_shape, -1])\n",
    "    y_pred = tf.reshape(y_pred, [batch_shape, -1])\n",
    "    \n",
    "    return tf.reduce_mean(tf.reduce_sum((y_true - y_pred)**2, axis=-1))\n",
    "\n",
    "\n",
    "def logloss(y_true, y_pred):\n",
    "    # this is what we get if assuming a gaussian likelihood and choosing optimal sigma\n",
    "    batch_shape = tf.shape(y_true)[0]\n",
    "    y_true = tf.reshape(y_true, [batch_shape, -1])\n",
    "    y_pred = tf.reshape(y_pred, [batch_shape, -1])\n",
    "    \n",
    "    return tf.reduce_mean(tf.math.log(tf.norm(y_true - y_pred, axis=-1)))\n",
    "\n",
    "\n",
    "def bernoulli_loss(y_true, y_pred):\n",
    "    # this is what we get if assuming a bernoulli likelihood\n",
    "    batch_shape = tf.shape(y_true)[0]\n",
    "    y_true = tf.reshape(y_true, [batch_shape, -1])\n",
    "    y_pred = tf.reshape(y_pred, [batch_shape, -1])\n",
    "    \n",
    "    xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    \n",
    "    return tf.reduce_mean(tf.reduce_sum(xent, axis=-1))\n",
    "\n",
    "\n",
    "def continuous_bernoulli_log_normalizer(lam, l_lim=0.49, u_lim=0.51):\n",
    "    # this is what we get if assuming a continuous bernoulli likelihood.\n",
    "    # taken from https://github.com/cunningham-lab/cb_and_cc\n",
    "    cut_lam = tf.where(tf.logical_or(tf.less(lam, l_lim), tf.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    log_norm = tf.math.log(tf.abs(2.0 * tf.math.atanh(1 - 2.0 * cut_lam))) - tf.math.log(tf.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = tf.math.log(2.0) + 4.0 / 3.0 * tf.pow(lam - 0.5, 2) + 104.0 / 45.0 * tf.pow(lam - 0.5, 4)\n",
    "    return tf.where(tf.logical_or(tf.less(lam, l_lim), tf.greater(lam, u_lim)), log_norm, taylor)\n",
    "\n",
    "\n",
    "def continuous_bernoulli_loss(y_true, y_pred):\n",
    "    # this is the loss for the continuous bernoulli distribution.\n",
    "    # it's really just binary cross-entropy plus one more term corresponding to the normalization constant\n",
    "    batch_shape = tf.shape(y_true)[0]\n",
    "    y_true = tf.reshape(y_true, [batch_shape, -1])\n",
    "    y_pred = tf.reshape(y_pred, [batch_shape, -1])\n",
    "    \n",
    "    base = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    add = continuous_bernoulli_log_normalizer(tf.clip_by_value(tf.nn.sigmoid(y_pred), 1e-4, 1 - 1e-4))\n",
    "    \n",
    "    return tf.reduce_mean(tf.reduce_sum(base - add, axis=-1))\n",
    "\n",
    "\n",
    "def continuous_bernoulli_expected_value(lam, l_lim=0.49, u_lim=0.51):\n",
    "    # our networks output distribution parameters, but we want the expected value.\n",
    "    # for gaussian and bernoulli distributions, the expected value is just equal to (one of) the\n",
    "    # distribution parameter(s).\n",
    "    # if using continuous bernoulli, the expected value is a bit more complicated\n",
    "    cut_lam = tf.where(tf.logical_or(tf.less(lam, l_lim), tf.greater(lam, u_lim)), lam, l_lim * tf.ones_like(lam))\n",
    "    expected = cut_lam / (2*cut_lam - 1) + 1 / (2*tf.math.atanh(1 - 2*cut_lam))\n",
    "    return tf.where(tf.logical_or(tf.less(lam, l_lim), tf.greater(lam, u_lim)), expected, 0.5*tf.ones_like(expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoders have target = input.\n",
    "# having to provide data as (image, image) tuples is annoying, so\n",
    "# I write a custom train step that does not require labels.\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructions = self(data, training=True)\n",
    "            \n",
    "            recon_loss = self.compiled_loss(data, reconstructions)\n",
    "            \n",
    "        variables = self.trainable_variables\n",
    "        gradients = tape.gradient(recon_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        self.compiled_metrics.update_state(data, reconstructions)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        reconstructions = self(data, training=False)\n",
    "\n",
    "        recon_loss = self.compiled_loss(data, reconstructions)\n",
    "        \n",
    "        self.compiled_metrics.update_state(data, reconstructions)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack(inputs, filters, strides, blocks_per_level, mode, name):\n",
    "    outputs = inputs\n",
    "    for level_ind, (level_filters, level_stride) in enumerate(zip(filters, strides)):\n",
    "        for block_ind in range(blocks_per_level):\n",
    "            outputs = ResidualBlock(level_filters,\n",
    "                                    mode, \n",
    "                                    strides=level_stride if block_ind == (blocks_per_level - 1) else 1,\n",
    "                                    name=\"_\".join([name, str(level_ind+1), str(block_ind+1)]))(outputs)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "blocks_per_level = 2\n",
    "filters = [64, 128, 256, 256]\n",
    "strides = [2, 2, 2, 1]\n",
    "\n",
    "# note, this architecture encodes 32,32,3 to 4,4,64\n",
    "# this is quite big compared to the input (downsampling by a factor of 3 only)\n",
    "# this architecture could probably use some more tuning :)\n",
    "encoder_input = tf.keras.Input((32, 32, 3))\n",
    "encoder_output = residual_stack(encoder_input, filters, strides, blocks_per_level, \"conv\", \"encoder\")\n",
    "encoder_final = tfkl.Conv2D(64, 1)(encoder_output)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoder_final, name=\"encoder\")\n",
    "code_shape = encoder.output_shape[1:]\n",
    "\n",
    "decoder_input = tf.keras.Input(code_shape)\n",
    "decoder_output = residual_stack(decoder_input, reversed(filters), strides, blocks_per_level, \"transpose\", \"decoder\")\n",
    "decoder_final = tfkl.Conv2D(3, 1)(decoder_output)\n",
    "\n",
    "decoder = tf.keras.Model(decoder_input, decoder_final, name=\"decoder\")\n",
    "\n",
    "model = Autoencoder(encoder_input, decoder(encoder(encoder_input)))\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loss_function = tf.losses.BinaryCrossentropy(from_logits=True)  # bernoulli likelihood, but AVERAGES over image dimensions\n",
    "loss_function = bernoulli_loss # bernoulli likelihood\n",
    "#loss_function = squared_loss  # gaussian likelihood with fixed sigma\n",
    "#loss_function = logloss  # gaussian likelihood with optimal sigma\n",
    "#loss_function = continuous_bernoulli_loss  # continuous bernoulli likelihood\n",
    "\n",
    "n_steps = 100000\n",
    "n_data = 60000\n",
    "n_epochs = n_steps // (n_data // batch_size)\n",
    "decay_function = tf.keras.optimizers.schedules.CosineDecay(0.001, n_steps)\n",
    "optimizer = tf.optimizers.Adam(decay_function)\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ReconstructionCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, frequency, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.frequency = frequency\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not epoch % self.frequency:\n",
    "            cropped_test = test_images[:32]  # TODO better not hardcode\n",
    "            # IF using gaussian likelihood: remove the sigmoid here\n",
    "            generated_batch = tf.nn.sigmoid(self.model(cropped_test)).numpy()\n",
    "            # IF using continuous bernoulli likelihood: add this line after the sigmoid\n",
    "            #generated_batch = continuous_bernoulli_expected_value(tf.clip_by_value(generated_batch, 1e-4, 1-1e-4)).numpy()\n",
    "        \n",
    "            plt.figure(figsize=(15,15))\n",
    "            for ind, (original, reconstruction) in enumerate(zip(cropped_test, generated_batch)):\n",
    "                comparison = np.concatenate((original, reconstruction), axis=1)\n",
    "                plt.subplot(8, 4, ind+1)\n",
    "                plt.imshow(comparison)\n",
    "                plt.axis(\"off\")\n",
    "            plt.suptitle(\"Test set reconstructions\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "do_train = True\n",
    "\n",
    "if do_train:\n",
    "    #lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5, verbose=1)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True, verbose=1)\n",
    "    reconstruct = ReconstructionCallback(10)  # plots reconstructions every 10 epochs\n",
    "\n",
    "    history = model.fit(train_data, epochs=n_epochs, validation_data=test_data,\n",
    "                        callbacks=[early_stop, reconstruct])\n",
    "    model.save_weights(\"weights/weights_assignment0.h5\")\n",
    "    # note: the if using the continuous bernoulli loss, the loss will likely be < 0.\n",
    "    # this might seem a bit weird, but is actually not an issue.\n",
    "    # recall that this loss is the negative log likelihood.\n",
    "    # - if the NLL is negative, that means the log likelihood is positive\n",
    "    # - if the log likelihood is > 0, that implies that the likelihood is > 1\n",
    "    # - a p > 1 is nothing unusual for continuous distributions, where we are using *density* functions\n",
    "else:\n",
    "    # instead of training, could load a previously trained model\n",
    "    model.load_weights(\"weights/weights_assignment0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line if you are using bernoulli likelihood\n",
    "reconstructions = tf.nn.sigmoid(model.predict(test_data)).numpy()\n",
    "# this line IN ADDITION to the above if you are using *continuous* bernoulli likelihood\n",
    "#reconstructions = continuous_bernoulli_expected_value(tf.clip_by_value(reconstructions, 1e-4, 1-1e-4)).numpy()\n",
    "\n",
    "# this line instead of the above, if you are using gaussian likelihood\n",
    "#reconstructions = model.predict(te_data)\n",
    "\n",
    "# compare some inputs and reconstructions\n",
    "plt.figure(figsize=(15, 15))\n",
    "for ind, (original, reconstruction) in enumerate(zip(test_images[:32], reconstructions[:32])):\n",
    "    plt.subplot(8, 4, ind+1)\n",
    "    concat = np.concatenate((original, reconstruction), axis=1)\n",
    "    plt.imshow(concat, vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting codes on the training set\n",
    "all_codes = encoder.predict(train_data).reshape((-1, np.prod(code_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some hamfisted attempts at generating random codes and applying the decoder\n",
    "def generate_images(method):\n",
    "    if method == \"uniform\":\n",
    "        random_codes = tf.random.uniform((64, np.prod(code_shape)), all_codes.min(axis=0), all_codes.max(axis=0))\n",
    "    elif method == \"gaussian\":\n",
    "        random_codes = tf.random.normal((64, np.prod(code_shape)), all_codes.mean(axis=0), np.std(all_codes, axis=0))\n",
    "    elif method == \"gaussian_full\":\n",
    "        covariance = np.cov(all_codes, rowvar=False).astype(np.float32)\n",
    "        distr = tfp.distributions.MultivariateNormalFullCovariance(all_codes.mean(axis=0), covariance)\n",
    "        random_codes = distr.sample(64)\n",
    "        \n",
    "    random_codes = tf.reshape(random_codes, (-1,) + code_shape)\n",
    "    \n",
    "    generated = tf.nn.sigmoid(decoder(random_codes)).numpy()\n",
    "    # again: the below is specific to using continuous bernoulli likelihood\n",
    "    #generated = continuous_bernoulli_expected_value(tf.clip_by_value(tf.nn.sigmoid(generated), 1e-4, 1-1e-4)).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for ind, img in enumerate(generated):\n",
    "        plt.subplot(8, 8, ind + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen = generate_images(\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen = generate_images(\"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen = generate_images(\"gaussian_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distribution of the first few code dimensions. looks reasonably gaussian!\n",
    "plt.figure(figsize=(15, 15))\n",
    "for ind, dim in enumerate(all_codes.T[:64]):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.hist(dim, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix reveals dependencies between dimensions\n",
    "covariance = np.cov(all_codes, rowvar=False).astype(np.float32)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "absmax = abs(covariance).max()\n",
    "plt.imshow(covariance[:64, :64], vmin=-absmax, vmax=absmax, cmap=\"coolwarm\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better: *correlation* matrix\n",
    "covariance = np.corrcoef(all_codes, rowvar=False).astype(np.float32)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "absmax = abs(covariance).max()\n",
    "plt.imshow(covariance[:64, :64], vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
