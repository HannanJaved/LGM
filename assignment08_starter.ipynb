{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first on the noise-conditional score model:\n",
    "# it could be a multi-input model like this\n",
    "\n",
    "# data shape: for example for padded MNIST (32x32 images with 1 channel)\n",
    "data_input = tf.keras.Input((32, 32, 1))\n",
    "# noise shape: convenient to have same number of dimensions for element-wise division later\n",
    "sigma_input = tf.keras.Input((1, 1, 1))\n",
    "\n",
    "score_output = Network(data_input)\n",
    "# this is the conditioning method proposed in the improved paper (technique 3)\n",
    "conditional_score_output = score_output / sigma_input\n",
    "\n",
    "score_model = tf.keras.Model([data_input, sigma_input], conditional_score_output)\n",
    "\n",
    "\n",
    "# ...and can be called like this\n",
    "sigma_batch = tf.repeat(sigma, tf.shape(noisy_data_batch)[0])[:, None, None, None]\n",
    "score_model([noisy_data_batch, sigma])\n",
    "\n",
    "# sigma_batch: take a sigma, repeat over batch axes, add axes for width, height, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb493fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the loss function:\n",
    "# use the loss from equation 2 in the improved paper https://arxiv.org/pdf/2006.09011.pdf\n",
    "\n",
    "# advanced note: compare this to equations 5 and 6 from the original https://arxiv.org/pdf/1907.05600.pdf\n",
    "# there, they have a weight lambda(sigma_i) for each noise level.\n",
    "# they propose lambda(sigma_i) = sigma_i**2.\n",
    "\n",
    "# in equation 2 of the advanced paper, they already inserted this weight and did some simplifications.\n",
    "# equation 2 involves sigma_i * conditional_score_model.\n",
    "\n",
    "# if we insert conditional_score_model = score_model / sigma_i, we have sigma_i * score_model / sigma_i.\n",
    "# we can thus remove sigma_i from the equation and are left with the unconditional score_model!\n",
    "\n",
    "# as such, you could implement the score model completely unconditionally, and ignore the noise_input.\n",
    "# you only have to remember to manually divide by sigma_i, when needed, e.g. for the langevin sampler.\n",
    "\n",
    "\n",
    "# if all of this is confusing to you, ignore it and stick with the conditional model from the 1st cell.\n",
    "\n",
    "\n",
    "# finally, you can use TF functions to sample sigma like this.\n",
    "# assuming you have a 1D tensor noise_scales_tensor that contains all the different sigmas.\n",
    "# syntax is a bit annoying -- we sample a random tensor of shape [1], then take the index [0] to get a scalar...\n",
    "random_index = tf.random.uniform([1], 0, len(sigmas_tensor), dtype=tf.int32)[0]\n",
    "random_sigma = sigmas_tensor[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0597fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rest is on choosing noise scales etc.\n",
    "# the improved paper proposes to use sigma_L = largest euclidean distance between data points.\n",
    "# actually computing this distance will take a long time.\n",
    "# but you can get a good approximation from a subset of data.\n",
    "# for MNIST I get around 16.3 or so.\n",
    "\n",
    "# let's just use 20\n",
    "biggest_distance = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many scales to choose. this is technique 3 from the paper.\n",
    "# ideally the value C computed at the bottom should be ~1.\n",
    "# but they say that aiming for a value such as 0.5 is an acceptable compromise.\n",
    "# generally, the smaller you make gamma, the larger C will get.\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "d = 32*32  # data dimensionality -- 32*32 for padded MNIST\n",
    "wish_gamma = 1.05  # <-- this is the target ratio between successive noise scales, try values > 1\n",
    "\n",
    "upper_limit = np.sqrt(2*d) * (wish_gamma - 1) + 3*wish_gamma\n",
    "lower_limit = np.sqrt(2*d) * (wish_gamma - 1) - 3*wish_gamma\n",
    "\n",
    "c_value = stats.norm.cdf(upper_limit) - stats.norm.cdf(lower_limit)\n",
    "\n",
    "print(\"C value is {}; should be 0.5 or higher! Too low? Make gamma smaller!\".format(c_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above, we got the wish_gamma that we would LIKE to have.\n",
    "# you could directly use that to define a noise scale.\n",
    "# or you use geomspace, but that doesn't allow for specification of gamma.\n",
    "# you can only say how many scales you want.\n",
    "\n",
    "# basically, the larger n_noise_scales, the smaller gamma will be (noises will be closer together).\n",
    "# increase n_noise_scales until gamma is at or below the target gamma you got above for a good C value.\n",
    "\n",
    "n_noise_scales = 200\n",
    "target_noise = 0.001\n",
    "noise_scales = np.geomspace(biggest_distance, target_noise, n_noise_scales, dtype=np.float32)\n",
    "true_gamma = noise_scales[0] / noise_scales[1]\n",
    "print(\"Gamma is {}, should be {} or lower! Too high? Make n_noise_scales larger!\".format(true_gamma, wish_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ed8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, technique 4 from the paper concerns the choice of epsilon for the langevin sampler.\n",
    "def amazing_formula(gamma, t, eps):\n",
    "    final_sig_sq = noise_scales[-1]**2\n",
    "    first = (1 - (eps / final_sig_sq))**(2*t)\n",
    "    second = gamma**2 - 2*eps / (final_sig_sq - final_sig_sq * (1 - eps/final_sig_sq)**2)\n",
    "    third = 2*eps / (final_sig_sq - final_sig_sq * (1 - eps/final_sig_sq)**2)\n",
    "    \n",
    "    return first*second + third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fe705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, set t_total to however many steps you can afford to run in total.\n",
    "# 1000 is probably on the lower side tbh.\n",
    "# also maybe choose it as a multiple of n_noise_scales\n",
    "\n",
    "t_total = 1000\n",
    "t_per_noise_scale = t_total // n_noise_scales\n",
    "\n",
    "# if you kept all the numbers in the notebook the same,\n",
    "# you likely won't get a much better epsilon than this. it should be around 1.07.\n",
    "# usually there is a \"global optimum\" for the some_value below.\n",
    "# both increasing or decreasing epsilon will increase the value.\n",
    "epsilon = 0.00000007\n",
    "some_value = amazing_formula(true_gamma, t_per_noise_scale, epsilon)\n",
    "\n",
    "print(\"The thingy value is {}! It should be close to 1! \"\n",
    "      \"Try playing around with the epsilon value.\".format(some_value))\n",
    "\n",
    "print(\"There will be {} iterations per noise scale.\".format(t_per_noise_scale))\n",
    "\n",
    "\n",
    "# \"close to 1\" is of course a relative statement. but e.g. if I remove two 0s from epsilon,\n",
    "# i.e. I use 0.000007, the some_value is around 90 million. that's not close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24639ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
