{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ff912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN training\n",
    "# this is pseudocode! don't expect it to run!\n",
    "def train_step(real_batch):\n",
    "    # train discriminator\n",
    "    generated_batch = generator(noise_function())\n",
    "    real_labels = ones(real_batch.shape)\n",
    "    generated_labels = zeros(generated_batch.shape)\n",
    "    \n",
    "    # concatenating is optional\n",
    "    full_batch = concat(real_batch, generated_batch)\n",
    "    full_labels = conact(real_labels, generated_labels)\n",
    "    # NOTE it's enough to start gradient tape here as we don't train G\n",
    "    d_output = discriminator(full_batch)\n",
    "    \n",
    "    # as always, be mindful of sigmoid and logits.\n",
    "    # i.e. for numerical stability it's good to use cross-entropy from_logits.\n",
    "    # then do not have a sigmoid in your discriminator output layer!\n",
    "    loss = crossentropy(full_labels, d_output)\n",
    "    compute_and_apply_gradients(loss, d_variables)\n",
    "    \n",
    "    \n",
    "    # train generator\n",
    "    # this time, you need the tape here already, since we backpropagate through G\n",
    "    generated_batch = generator(noise_function())\n",
    "    fake_labels = ones(generated_batch.shape)  # ones this time!!\n",
    "    d_output = discriminator(generated_batch)\n",
    "    \n",
    "    # this implements the \"flipped loss\" which has better gradients for G\n",
    "    loss = crossentropy(fake_labels, d_output)\n",
    "    compute_and_apply_gradients(loss, g_variables)\n",
    "    \n",
    "\n",
    "# here are some ideas for further \"tricks\" that MAY help\n",
    "\n",
    "# (one-sided) label-smoothing\n",
    "# - use soft labels such as 0.1 instead of 0, and 0.9 instead of 1\n",
    "#   - in the one-sided version, it is proposed to only smooth the 1 label.\n",
    "#     also, ONLY smooth it when training D, not when training G.\n",
    "\n",
    "# normalize data into [-1, 1] instead of [0, 1]\n",
    "# - accordingly, use tanh as output activation in G instead of sigmoid.\n",
    "# - seems like it should do nothing, but supposedly it helps with the learning dynamics.\n",
    "\n",
    "# \"dequantize\" the training data\n",
    "# - just add noise! in theory, uniform noise with range of +/- 1 pixel\n",
    "# - so. if your data is in [0, 1], +/- 1/256.\n",
    "\n",
    "# DO NOT use batchnorm in the discriminator. DEFINITELY not in the first layer.\n",
    "# - you can use e.g. LayerNormalization, or InstanceNormalization, GroupNormalization from tensorflow_addons.\n",
    "# - batchnorm in the generator is okay.\n",
    "# if you are interested in this issue, you could read this https://ovgu-ailab.github.io/blog/methods/2022/07/07/batchnorm-gans.html\n",
    "\n",
    "# feature matching is great! see the paper \"Improved Techniques for Training GANs\",\n",
    "# section 3.1: https://arxiv.org/pdf/1606.03498.pdf\n",
    "# here is very rough sketch how it may be implemented\n",
    "\n",
    "# again this is just pseudocode!\n",
    "# discriminator definition\n",
    "inputs = tf.keras.Input(noise_shape)\n",
    "h1 = layer(inputs)\n",
    "h2 = layer(h1)\n",
    "output = layer(h3)\n",
    "\n",
    "# now don't do this\n",
    "discriminator = Model(inputs, output)\n",
    "# ...do this instead!\n",
    "# there is no theory on which hidden layer(s) to pick.\n",
    "# you can choose one, or multiple and compute feature matching for all chosen layers, and add up the losses.\n",
    "discriminator = Model(input, [output, h1, h2])\n",
    "\n",
    "# training D proceeds as normal\n",
    "outputs = discriminator(batch)\n",
    "logits = outputs[0]  # take final layer output, discard hidden layers\n",
    "...\n",
    "\n",
    "# feature matching is only used for training G\n",
    "real_outputs = discriminator(real_batch)\n",
    "real_features = real_outputs[1:]  # discard logits, keep hidden layer outputs\n",
    "fake_outputs = discriminator(generated_batch)\n",
    "fake_features = fake_outputs[1:]\n",
    "\n",
    "total_loss = 0\n",
    "for real_feature, fake_feature in zip(real_features, fake_features):\n",
    "    # monte carlo approximation of expectations, AKA mean over batch\n",
    "    squared_difference = (mean(real_feature, axis=0) - mean(fake_feature, axis=0))**2 \n",
    "    total_loss += sum(squared_difference)  # sum over feature dimensions\n",
    "    \n",
    "# you can also still train G on the normal loss (just add it), but I dunno if it really makes a difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
