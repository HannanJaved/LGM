{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this cell when running on colab\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this as well\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some utilities\"\"\"\n",
    "\n",
    "def repeated_gibbs(initial_sample, n_iterations, gibbs_update_fn,\n",
    "                   return_all=False, **guf_kwargs):\n",
    "    \"\"\"Repeatedly apply Gibbs updates for a given number of iterations.\n",
    "    \n",
    "    Alternative wording: Run a Markov chain.\n",
    "\n",
    "    Parameters:\n",
    "        initial_sample: Batch of input samples to start with. Needs to be in the\n",
    "                     appropriate format for the update function (e.g. tuple of\n",
    "                     visible/hidden for RBMs).\n",
    "        n_iterations: How many Gibbs updates to do.\n",
    "        gibbs_update_fn: Function that takes a batch of input samples and\n",
    "                         computes a new one.\n",
    "        return_all: If true, return all samples, not just the last one. Can be used\n",
    "                    if you want to plot the entire chain, for example.\n",
    "        guf_kwargs: Keyword arguments passed to gibbs_update_fn.\n",
    "\n",
    "    Returns:\n",
    "        New batch of input samples.\n",
    "\n",
    "    \"\"\"\n",
    "    iteration_dummy = tf.range(n_iterations)\n",
    "\n",
    "    def loop_body(sample, _): return gibbs_update_fn(sample, **guf_kwargs)\n",
    "\n",
    "    loop_fn = tf.scan if return_all else tf.foldl\n",
    "    return loop_fn(loop_body, iteration_dummy, initializer=initial_sample, back_prop=False)\n",
    "\n",
    "\n",
    "def repeated_gibbs_python(initial_sample, n_iterations, gibbs_update_fn,\n",
    "                          return_all=False, **guf_kwargs):\n",
    "    \"\"\"Included for pedagogical reasons. ;)\n",
    "    \n",
    "    It does the same thing as the above function, but perhaps easier to understand using Python loops.\n",
    "    \n",
    "    The only annoying this is that, for this to work properly in a tf.function, we need to use\n",
    "    TensorArrays for iteration. But for the case return_all=False, you can ignore them completely.\n",
    "    \"\"\"\n",
    "    if return_all:\n",
    "        visible_samples = tf.TensorArray(tf.float32, size=n_iterations+1)\n",
    "        hidden_samples = tf.TensorArray(tf.float32, size=n_iterations+1)\n",
    "        visible_samples = visible_samples.write(0, initial_sample[0])\n",
    "        hidden_samples = hidden_samples.write(0, initial_sample[1])\n",
    "        \n",
    "    # \"core logic\" starts here\n",
    "    sample = initial_sample\n",
    "    for index in tf.range(n_iterations):\n",
    "        sample = gibbs_update_fn(sample, **guf_kwargs)\n",
    "    # \"core logic\" ends here\n",
    "        \n",
    "        if return_all:\n",
    "            visible_samples = visible_samples.write(index + 1, sample[0])\n",
    "            hidden_samples = hidden_samples.write(index + 1, sample[1])\n",
    "\n",
    "    return (visible_samples.stack(), hidden_samples.stack()) if return_all else sample\n",
    "\n",
    "\n",
    "def gibbs_update_brbm(previous_sample, w_vh, b_v, b_h):\n",
    "    \"\"\"Gibbs update step for binary RBMs.\n",
    "\n",
    "    Given an input sample, take a hidden sample and then a new input sample.\n",
    "\n",
    "    Parameters:\n",
    "        prev_sample: Tuple of b x d_v tensor and b x d_h tensor: Both batches\n",
    "                     of input/hidden samples.\n",
    "        w_vh: Connection matrix of RBM, d_v x d_h.\n",
    "        b_v: Bias vector for inputs, d_v-dimensional.\n",
    "        b_h: Bias vector for hidden variables, d_h-dimensional.\n",
    "\n",
    "    Returns:\n",
    "        New batch of input/hidden samples as tuple.\n",
    "\n",
    "    \"\"\"\n",
    "    v, _ = previous_sample\n",
    "\n",
    "    p_h_given_v = tf.nn.sigmoid(tf.matmul(v, w_vh) + b_h)\n",
    "    sample_h = tfp.distributions.Bernoulli(\n",
    "        probs=p_h_given_v, dtype=tf.float32).sample()\n",
    "    \n",
    "    p_v_given_h = tf.nn.sigmoid(tf.matmul(sample_h, tf.transpose(w_vh)) + b_v)\n",
    "    sample_v = tfp.distributions.Bernoulli(\n",
    "        probs=p_v_given_h, dtype=tf.float32).sample()\n",
    "\n",
    "    return sample_v, sample_h\n",
    "\n",
    "\n",
    "def energy_rbm(v, h, w_vh, b_v, b_h):\n",
    "    \"\"\"Compute energy for an RBM.\n",
    "\n",
    "    Parameters:\n",
    "        v: Batch of inputs, b x d_v.\n",
    "        h: Batch of hidden units, b x d_h.\n",
    "        w_vh: Connection matrix of RBM, d_v x d_h.\n",
    "        b_v: Bias vector for inputs, d_v-dimensional.\n",
    "        b_h: Bias vector for hidden variables, d_h-dimensional.\n",
    "\n",
    "    Returns:\n",
    "        b-dimensional vector, energy for each batch element.\n",
    "\n",
    "    \"\"\"\n",
    "    return (-tf.linalg.matvec(v, b_v) - tf.linalg.matvec(h, b_h) \n",
    "            - tf.reduce_sum(tf.matmul(v, w_vh) * h, axis=-1))  # alternative: batched matmul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data\"\"\"\n",
    "\n",
    "# data\n",
    "batch_size = 1024\n",
    "n_v = 28*28\n",
    "\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((-1, n_v)).astype(np.float32) / 255.\n",
    "test_images = test_images.reshape((-1, n_v)).astype(np.float32) / 255.\n",
    "\n",
    "# binarize data\n",
    "train_images = (train_images >= 0.5).astype(np.float32)\n",
    "test_images = (test_images >= 0.5).astype(np.float32)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size).repeat()\n",
    "\n",
    "# compute marginal for better sampling\n",
    "marginals = tf.reduce_mean(train_images, axis=0)\n",
    "\n",
    "start_sampler = tfd.Bernoulli(probs=0.5, dtype=tf.float32)\n",
    "marginal_sampler = tfd.Bernoulli(probs=marginals, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows, for each pixel position, the probability to sample it as 1 if using marginal_sampler\n",
    "plt.imshow(marginals.numpy().reshape(28, 28), cmap=\"Greys\", vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "mode = \"simple\"  # simple is the \"naive\" algorithm 18.1\n",
    "if mode not in [\"pcd\", \"cd\", \"simple\"]:\n",
    "    raise ValueError(\"Invalid mode!\")\n",
    "\n",
    "n_h = 512  # could be tuned\n",
    "w_vh = tf.Variable(tf.random.uniform([n_v, n_h], -0.1, 0.1))\n",
    "b_v = tf.Variable(tf.zeros([n_v]))\n",
    "b_h = tf.Variable(tf.zeros([n_h]))\n",
    "weights = [w_vh, b_v, b_h]\n",
    "\n",
    "train_steps = 5000\n",
    "chain_length = 500  # how long to run Markov chains\n",
    "\n",
    "# SGD with decay worked much better for me than adam\n",
    "optimizer = tf.optimizers.SGD(tf.keras.optimizers.schedules.PolynomialDecay(0.1, train_steps, 1e-3))\n",
    "#optimizer = tf.optimizers.Adam() \n",
    "\n",
    "# if true, initial samples for v are taken from the marginal distribution above.\n",
    "# if false, we just sample each pixel randomly with p=0.5.\n",
    "sample_from_marginal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def train(batch, v_sampled=None, h_sampled=None):\n",
    "    \"\"\"v_sampled and h_sampled are used only for PCD.\n",
    "    \n",
    "    It's always passed because I'm lazy.\n",
    "    \"\"\"\n",
    "    v_data = batch\n",
    "    h_data = tf.nn.sigmoid(tf.matmul(v_data, w_vh) + b_h)\n",
    "    h_data = tfd.Bernoulli(probs=h_data, dtype=tf.float32).sample()\n",
    "    \n",
    "    # gibbs sampling -- naive/cd/pcd only differ in how the chains are initialized\n",
    "    if mode == \"cd\":\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_data, h_data), chain_length//10, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "        \n",
    "    elif mode == \"pcd\":\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_sampled, h_sampled), chain_length//10, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "        \n",
    "    else:\n",
    "        if sample_from_marginal:\n",
    "            v_random = marginal_sampler.sample(tf.shape(batch)[0])\n",
    "        else:\n",
    "            v_random = start_sampler.sample([tf.shape(batch)[0], n_v])\n",
    "        # h_random is just a dummy, it will immediately be overwritten by a sample conditioned \n",
    "        # on v, but the code is set up such that it needs an initial value for h...\n",
    "        h_random = start_sampler.sample([tf.shape(batch)[0], n_h])\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_random, h_random), chain_length, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "\n",
    "    # compute \"loss\" and take the gradient\n",
    "    # altough this loss can take any value (real number), it should converge to around 0\n",
    "    # if it goes to -infinity there is something wrong!\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_positive = tf.reduce_mean(-energy_rbm(v_data, h_data, w_vh, b_v, b_h))\n",
    "        logits_negative = tf.reduce_mean(\n",
    "            -energy_rbm(v_sampled, h_sampled, w_vh, b_v, b_h))\n",
    "        loss = -(logits_positive - logits_negative)\n",
    "    gradients = tape.gradient(loss, weights)\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n",
    "    \n",
    "    # returning the sampled values is once again for PCD only\n",
    "    return loss, v_sampled, h_sampled\n",
    "\n",
    "\n",
    "# training loop\n",
    "if sample_from_marginal:\n",
    "    v_sampled = marginal_sampler.sample(batch_size)\n",
    "else:\n",
    "    v_sampled = start_sampler.sample([batch_size, n_v])\n",
    "h_sampled = start_sampler.sample([batch_size, n_h])\n",
    "for step, img_batch in enumerate(data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    loss, v_sampled, h_sampled = train(img_batch, v_sampled, h_sampled)\n",
    "    if not step % 250:\n",
    "        print(\"Step\", step)\n",
    "        print(\"Loss:\", loss) \n",
    "        \n",
    "        # look at some samples\n",
    "        if sample_from_marginal:\n",
    "            v_random = marginal_sampler.sample([64])\n",
    "        else:\n",
    "            v_random = start_sampler.sample([64, n_v])\n",
    "        h_random = start_sampler.sample([64, n_h])\n",
    "        image_sample, final_h = repeated_gibbs(\n",
    "            (v_random, h_random), chain_length, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "\n",
    "        # these are the \"true\" binary samples\n",
    "        image_sample = [image.numpy().reshape((28, 28)) for image in image_sample]\n",
    "\n",
    "        f = plt.figure(figsize=(15, 15))\n",
    "        f.suptitle(\"binary samples\")\n",
    "        for ind, image in enumerate(image_sample):\n",
    "            plt.subplot(8, 8, ind+1)\n",
    "            plt.imshow(image, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # here, we plot the _probabilities_ of v in the last step (at the end of the markov chain)\n",
    "        # much smoother, less noisy due to lack of randomness\n",
    "        images_prob = tf.nn.sigmoid(tf.matmul(final_h, tf.transpose(w_vh)) + b_v)\n",
    "        image_sample = [image.numpy().reshape((28, 28)) for image in images_prob]\n",
    "\n",
    "        f = plt.figure(figsize=(15, 15))\n",
    "        f.suptitle(\"probabilities\")\n",
    "        for ind, image in enumerate(image_sample):\n",
    "            plt.subplot(8, 8, ind+1)\n",
    "            plt.imshow(image, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"random samples\"\"\"\n",
    "if sample_from_marginal:\n",
    "    v_random = marginal_sampler.sample([64])\n",
    "else:\n",
    "    v_random = start_sampler.sample([64, n_v])\n",
    "h_random = start_sampler.sample([64, n_h])\n",
    "image_sample, final_h = repeated_gibbs(\n",
    "    (v_random, h_random), chain_length, gibbs_update_brbm,\n",
    "    w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "\n",
    "# these are the \"true\" binary samples\n",
    "image_sample = [image.numpy().reshape((28, 28)) for image in image_sample]\n",
    "\n",
    "f = plt.figure(figsize=(15, 15))\n",
    "f.suptitle(\"binary samples\")\n",
    "for ind, image in enumerate(image_sample):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.imshow(image, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# here, we plot the _probabilities_ of v in the last step (at the end of the markov chain)\n",
    "# much smoother, less noisy due to lack of randomness\n",
    "images_prob = tf.nn.sigmoid(tf.matmul(final_h, tf.transpose(w_vh)) + b_v)\n",
    "image_sample = [image.numpy().reshape((28, 28)) for image in images_prob]\n",
    "\n",
    "f = plt.figure(figsize=(15, 15))\n",
    "f.suptitle(\"probabilities\")\n",
    "for ind, image in enumerate(image_sample):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.imshow(image, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something we can do for a sort of \"introspection\":\n",
    "# there is one bias per pixel, so we can shape it into an image and plot.\n",
    "# this shows the general \"preference\" of the model to activate certain pixels\n",
    "bias_image = b_v.numpy().reshape((28, 28))\n",
    "plt.imshow(bias_image, cmap=\"Greys_r\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, each hidden unit is connected to each visible unit, so we can do the same thing.\n",
    "# each unit encodes an input pattern that would maximally activate it.\n",
    "weight_images = w_vh.numpy().T.reshape((-1, 28, 28))\n",
    "\n",
    "\n",
    "absmax = abs(weight_images).max()\n",
    "plt.figure(figsize=(15, 15))\n",
    "# here, only for a selection of units (64 of them)\n",
    "for ind in range(64):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.imshow(weight_images[ind], vmin=-absmax, vmax=absmax, cmap=\"coolwarm\")\n",
    "    plt.axis(\"off\")\n",
    "# I dunno how to fix the colorbar xd\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this cell takes quite long!!\n",
    "\n",
    "import os\n",
    "\n",
    "# another thing we can do: plot the development of the chain over time.\n",
    "plot_frequency = 25\n",
    "# optionally we can create a GIF of the chains. to do that, set a folder name here.\n",
    "gif_folder = None\n",
    "if gif_folder and not os.path.exists(gif_folder):\n",
    "    os.mkdir(gif_folder)\n",
    "\n",
    "if sample_from_marginal:\n",
    "    v_random = marginal_sampler.sample([64])\n",
    "else:\n",
    "    v_random = start_sampler.sample([64, n_v])\n",
    "h_random = start_sampler.sample([64, n_h])\n",
    "image_sample, _ = repeated_gibbs(\n",
    "    (v_random, h_random), chain_length, gibbs_update_brbm,\n",
    "    return_all=True, w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "\n",
    "f = plt.figure(figsize=(15, 15))\n",
    "for ind, image in enumerate(v_random):\n",
    "    plt.subplot(8, 8, ind+1)\n",
    "    plt.imshow(image.numpy().reshape((28, 28)), cmap=\"Greys\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "if gif_folder:\n",
    "    plt.savefig(os.path.join(gif_folder, \"step0.png\"))\n",
    "plt.show()\n",
    "\n",
    "for ii, chain_states in enumerate(image_sample):\n",
    "    chain_states = [image.numpy().reshape((28, 28)) for image in chain_states]\n",
    "    f = plt.figure(figsize=(15, 15))\n",
    "    if gif_folder is None:  # don't want title in the images if making a gif\n",
    "        f.suptitle(\"Step {}\".format(ii+1))\n",
    "        \n",
    "    for ind, image in enumerate(chain_states):\n",
    "        plt.subplot(8, 8, ind+1)\n",
    "        plt.imshow(image, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    if gif_folder:\n",
    "        plt.savefig(os.path.join(gif_folder, \"step{}.png\".format(ii+1)))\n",
    "    if not ii % plot_frequency:\n",
    "        plt.show()\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for creating a gif from the images saved above.\n",
    "# you may need to !pip install imageio\n",
    "import imageio\n",
    "filenames = [os.path.join(gif_folder, \"step{}.png\".format(step)) for step in range(chain_length+1)]\n",
    "with imageio.get_writer(os.path.join(gif_folder, \"markov_chains.gif\"), mode='I') as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus 1\n",
    "# This is an alternative formulation with hand-derived gradients rather than using gradient tape.\n",
    "# it seems to be slower than the one above... it's probably possible to do the computations for W\n",
    "# more efficiently.\n",
    "\n",
    "# this is basically a drop-in replacement for cell #8 (the one with the other train function).\n",
    "# so run this cell INSTEAD of the above one.\n",
    "# this could also be integrated into the other cell like\n",
    "#\n",
    "# if use_tape:\n",
    "#     train_with_GradientTape\n",
    "# else:\n",
    "#     train_with_manual_gradients\n",
    "#\n",
    "# ...since nothing else changes\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_manual(batch, lr, v_sampled=None, h_sampled=None):\n",
    "    \"\"\"v_sampled and h_sampled are used only for PCD.\n",
    "    \n",
    "    It's always passed because I'm lazy.\n",
    "    \"\"\"\n",
    "    v_data = batch\n",
    "    h_data = tf.nn.sigmoid(tf.matmul(v_data, w_vh) + b_h)\n",
    "    h_data = tfd.Bernoulli(probs=h_data, dtype=tf.float32).sample()\n",
    "    \n",
    "    if mode == \"cd\":\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_data, h_data), chain_length//10, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "    elif mode == \"pcd\":\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_sampled, h_sampled), chain_length//10, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "    else:\n",
    "        v_random = marginal_sampler.sample(tf.shape(batch)[0])\n",
    "        #v_random = start_sampler.sample(tf.shape(batch))\n",
    "        # this is just a dummy\n",
    "        h_random = start_sampler.sample([tf.shape(batch)[0], n_h])\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_random, h_random), chain_length, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "    \n",
    "    # next lines are not necessary -- only computing this term to get an\n",
    "    # idea of how training is going\n",
    "    logits_pos = tf.reduce_mean(-energy_rbm(v_data, h_data, w_vh, b_v, b_h))\n",
    "    logits_neg = tf.reduce_mean(\n",
    "        -energy_rbm(v_sampled, h_sampled, w_vh, b_v, b_h))\n",
    "    loss = -(logits_pos - logits_neg)\n",
    "        \n",
    "    # compute gradients of (negative!) energy for data and samples\n",
    "    # each term is gradient for data minus gradient for model samples\n",
    "    # note that even computing two means per line is unnecessary, terms could be subtracted\n",
    "    # inside the mean. but that might get confusing.\n",
    "    b_v_grads = tf.reduce_mean(v_data - v_sampled, axis=0)\n",
    "    b_h_grads = tf.reduce_mean(h_data - h_sampled, axis=0)\n",
    "    w_grads = tf.reduce_mean(v_data[:, :, None] * h_data[:, None, :]\n",
    "                             - v_sampled[:, :, None] * h_sampled[:, None, :], axis=0)\n",
    "    \n",
    "    b_v.assign_add(lr * b_v_grads)\n",
    "    b_h.assign_add(lr * b_h_grads)\n",
    "    w_vh.assign_add(lr * w_grads)\n",
    "    \n",
    "    return loss, v_sampled, h_sampled\n",
    "\n",
    "\n",
    "v_samp = marginal_sampler.sample(batch_size)\n",
    "#v_samp = start_sampler.sample([batch_size, 1024])\n",
    "h_samp = start_sampler.sample([batch_size, n_h])\n",
    "for step, img_batch in enumerate(data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "        \n",
    "    # note we compute the learning rate decay by hand and pass it to the train function\n",
    "    lr = (0.1 - 1e-3) * (1-step/train_steps) + 1e-3\n",
    "\n",
    "    loss, v_samp, h_samp = train_manual(img_batch, tf.convert_to_tensor(lr), v_samp, h_samp)\n",
    "    if not step % 50:\n",
    "        print(\"Step\", step)\n",
    "        print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bonus #2: pseudolikelihood? this was optional reading.\n",
    "# another training-replacement-cell.\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def train(batch):\n",
    "    # get a set of hidden values\n",
    "    h_sample = tf.nn.sigmoid(tf.matmul(batch, w_vh) + b_h)\n",
    "    h_sample = tfp.distributions.Bernoulli(probs=h_sample, dtype=tf.float32).sample()\n",
    "    with tf.GradientTape() as tape:\n",
    "        # in pseudolikelihood we should compute the probability of each variable, conditioned\n",
    "        # on all the other variables. the bipartite RBM structure helps once again:\n",
    "        # we can compute all v conditionals and all h conditionals at once, respectively\n",
    "        # -> only 2 steps instead of n_hidden + n_visible steps\n",
    "        v_probs = tf.nn.sigmoid((2*batch-1) * (tf.matmul(h_sample, tf.transpose(w_vh)) + b_v))\n",
    "        h_probs = tf.nn.sigmoid((2*h_sample-1) * (tf.matmul(batch, w_vh) + b_h))\n",
    "        \n",
    "        pll = -1*tf.reduce_mean(tf.reduce_sum(tf.math.log(v_probs), axis=-1) + \n",
    "                                tf.reduce_sum(tf.math.log(h_probs), axis=-1))\n",
    "    grads = tape.gradient(pll, weights)\n",
    "    optimizer.apply_gradients(zip(grads, weights))\n",
    "    \n",
    "    return pll\n",
    "\n",
    "# you probably want to change the decay schedule of the optimizer to work over more steps!!\n",
    "# pseudolikelihood seems to take many steps to converge. maybe use 10 times as many.\n",
    "# but because we don't run markov chains in training, each step is MUCH faster\n",
    "for step, img_batch in enumerate(data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    loss = train(img_batch)\n",
    "    if not step % 5000:\n",
    "        # for sampling, we still use a markov chain.\n",
    "        # at least I don't know any other way xd\n",
    "        print(\"Step\", step)\n",
    "        print(\"Loss:\", loss)\n",
    "        #v_random = marginal_sampler.sample([64])\n",
    "        v_random = start_sampler.sample([64, n_v])\n",
    "        h_random = start_sampler.sample([64, n_h])\n",
    "        img_sample, final_h = repeated_gibbs(\n",
    "                (v_random, h_random), chain_length, gibbs_update_brbm,\n",
    "                w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "\n",
    "        # these are the \"true\" binary samples\n",
    "        img_sample = [img.numpy().reshape((28, 28)) for img in img_sample]\n",
    "\n",
    "        f = plt.figure(figsize=(15, 15))\n",
    "        f.suptitle(\"binary samples\")\n",
    "        for ind, img in enumerate(img_sample):\n",
    "            plt.subplot(8, 8, ind+1)\n",
    "            plt.imshow(img, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # here, we plot the _probabilities_ of v in the last step (at the end of the markov chain)\n",
    "        # much smoother, less noisy due to lack of randomness\n",
    "        imgs_p = tf.nn.sigmoid(tf.matmul(final_h, tf.transpose(w_vh)) + b_v)\n",
    "        img_sample = [img.numpy().reshape((28, 28)) for img in imgs_p]\n",
    "\n",
    "        f = plt.figure(figsize=(15, 15))\n",
    "        f.suptitle(\"probabilities\")\n",
    "        for ind, img in enumerate(img_sample):\n",
    "            plt.subplot(8, 8, ind+1)\n",
    "            plt.imshow(img, cmap=\"Greys\", vmin=0, vmax=1)\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "# once this is trained, you \"should\" be able to draw samples via gibbs sampling just like\n",
    "# before (cells above). but for me it didn't work well :( I only got 0s as samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
