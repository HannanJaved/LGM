{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 7: The NICEst Assignment - Kai Ponel & Hannan Mahadik "
      ],
      "metadata": {
        "id": "3QeHqwBvA0uo"
      },
      "id": "3QeHqwBvA0uo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom"
      ],
      "metadata": {
        "id": "jg0NAdLlPnxR"
      },
      "id": "jg0NAdLlPnxR"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "tfd = tfp.distributions\n",
        "\n",
        "from tensorflow.keras.layers import Layer, Dense, LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WCrI_hmOP3wu"
      },
      "id": "WCrI_hmOP3wu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NICECouplingLayer(Layer):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(NICECouplingLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.network = self.build_network()\n",
        "\n",
        "    def build_network(self):\n",
        "        input_layer = tf.keras.Input(shape=(self.input_dim,))\n",
        "        hidden_layer = LayerNormalization()(input_layer)\n",
        "        hidden_layer = Dense(self.hidden_dim, activation='relu')(hidden_layer)\n",
        "        hidden_layer = LayerNormalization()(hidden_layer)\n",
        "        output_layer = Dense(self.input_dim)(hidden_layer)\n",
        "\n",
        "        model = tf.keras.Model(input_layer, output_layer)\n",
        "        return model\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x1, x2 = tf.split(inputs, 2, axis=1)\n",
        "\n",
        "        if training:\n",
        "            y1 = x1\n",
        "            y2 = x2 + self.network(x1)\n",
        "        else:\n",
        "            y1 = x1\n",
        "            y2 = x2 - self.network(x1)\n",
        "\n",
        "        return tf.concat([y1, y2], axis=1)"
      ],
      "metadata": {
        "id": "a0ny7DJ3PrsO"
      },
      "id": "a0ny7DJ3PrsO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NICEModel(Layer):\n",
        "    def __init__(self, input_dim, hidden_dim, num_coupling_layers):\n",
        "        super(NICEModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_coupling_layers = num_coupling_layers\n",
        "        self.coupling_layers = self.build_coupling_layers()\n",
        "        self.diagonal_scale = self.add_weight(shape=(self.input_dim,),\n",
        "                                              initializer='ones',\n",
        "                                              trainable=True)\n",
        "\n",
        "    def build_coupling_layers(self):\n",
        "        layers = []\n",
        "        for _ in range(self.num_coupling_layers):\n",
        "            layers.append(NICECouplingLayer(self.input_dim // 2, self.hidden_dim))\n",
        "            layers.append(NICECouplingLayer(self.input_dim // 2, self.hidden_dim))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.coupling_layers:\n",
        "            x = layer(x, training=True)\n",
        "        return x * self.diagonal_scale\n",
        "\n",
        "    def backward(self, y):\n",
        "        y = y / self.diagonal_scale\n",
        "        for layer in reversed(self.coupling_layers):\n",
        "            y = layer(y, training=False)\n",
        "        return y"
      ],
      "metadata": {
        "id": "nKdzCn_biJ-a"
      },
      "id": "nKdzCn_biJ-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 10  # Dimensionality of the input\n",
        "hidden_dim = 1024  # Dimensionality of the hidden layer\n",
        "num_coupling_layers = 4  # Number of coupling layers in the model\n",
        "\n",
        "# Create the NICE model\n",
        "nice_model = NICEModel(input_dim, hidden_dim, num_coupling_layers)\n",
        "\n",
        "# Generate a random input tensor\n",
        "input_tensor = tf.random.normal((1, input_dim))\n",
        "\n",
        "# Forward pass through the NICE model\n",
        "output_tensor = nice_model.forward(input_tensor)\n",
        "print(\"Forward Output Tensor:\", output_tensor)\n",
        "\n",
        "# Inverse pass through the NICE model\n",
        "reconstructed_tensor = nice_model.backward(output_tensor)\n",
        "print(\"Inverse Reconstructed Tensor:\", reconstructed_tensor)\n",
        "\n",
        "# Calculate the difference between the original input and the reconstructed input\n",
        "difference = tf.reduce_sum(tf.abs(input_tensor - reconstructed_tensor))\n",
        "print(\"Difference:\", difference.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gSOr8m1iLmZ",
        "outputId": "36f78344-b5cb-4995-c332-6b06583887c6"
      },
      "id": "0gSOr8m1iLmZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Output Tensor: tf.Tensor(\n",
            "[[-0.9032111   0.52081865  1.2737176  -0.6568407  -0.5113207   1.5330367\n",
            "   1.4914765  -7.0619383   5.7860475  -1.5632026 ]], shape=(1, 10), dtype=float32)\n",
            "Inverse Reconstructed Tensor: tf.Tensor(\n",
            "[[-0.9032111   0.52081865  1.2737176  -0.6568407  -0.5113207   0.19184715\n",
            "   0.26891977 -0.88015985  1.2373682   0.76522535]], shape=(1, 10), dtype=float32)\n",
            "Difference: 5.5134296e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 2048\n",
        "x2_dist = tfd.Normal(loc=0., scale=0.5)\n",
        "x2_samples = x2_dist.sample(n_samples)\n",
        "x1 = tfd.Normal(loc=1. * tf.square(x2_samples),\n",
        "                scale=0.1*tf.ones(n_samples, dtype=tf.float32))\n",
        "x1_samples = x1.sample()\n",
        "x_samples = tf.stack([x1_samples, x2_samples], axis=1)\n",
        "\n",
        "as_np = x_samples.numpy()\n",
        "plt.scatter(as_np[:, 0], as_np[:, 1])\n",
        "a = plt.gca()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "qzYj6kRgKEJ8",
        "outputId": "9d95b5cd-db8d-41ad-c6d3-5d51df8285ab"
      },
      "id": "qzYj6kRgKEJ8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 2  # Dimensionality of the input\n",
        "hidden_dim = 1024  # Dimensionality of the hidden layer\n",
        "num_coupling_layers = 4  # Number of coupling layers in the model\n",
        "\n",
        "# Create the NICE model\n",
        "nice_model = NICEModel(input_dim, hidden_dim, num_coupling_layers)"
      ],
      "metadata": {
        "id": "kicryJNWUiY-"
      },
      "id": "kicryJNWUiY-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = tf.convert_to_tensor(x_samples, dtype=tf.float32)\n",
        "\n",
        "# Forward pass through the NICE model\n",
        "output_tensor = nice_model.forward(input_tensor)\n",
        "print(\"Forward Output Tensor:\", output_tensor)\n",
        "\n",
        "# Inverse pass through the NICE model\n",
        "reconstructed_tensor = nice_model.backward(output_tensor)\n",
        "print(\"Inverse Reconstructed Tensor:\", reconstructed_tensor)\n",
        "\n",
        "# Calculate the difference between the original input and the reconstructed input\n",
        "difference = tf.reduce_sum(tf.abs(input_tensor - reconstructed_tensor))\n",
        "print(\"Difference:\", difference.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-ik33lJRmBT",
        "outputId": "e6f3f878-6068-4dc0-86ab-f2617c409c4e"
      },
      "id": "2-ik33lJRmBT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Output Tensor: tf.Tensor(\n",
            "[[ 0.19697177  0.3914664 ]\n",
            " [ 0.12130839 -0.3628856 ]\n",
            " [-0.09612302  0.26519707]\n",
            " ...\n",
            " [-0.02931317  0.13351545]\n",
            " [ 0.03520637  0.03869548]\n",
            " [ 0.00844221 -0.234889  ]], shape=(2048, 2), dtype=float32)\n",
            "Inverse Reconstructed Tensor: tf.Tensor(\n",
            "[[ 0.19697177  0.39146668]\n",
            " [ 0.12130839 -0.36288556]\n",
            " [-0.09612302  0.26519707]\n",
            " ...\n",
            " [-0.02931317  0.13351548]\n",
            " [ 0.03520637  0.0386955 ]\n",
            " [ 0.00844221 -0.234889  ]], shape=(2048, 2), dtype=float32)\n",
            "Difference: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "U1PL-ATQiT69"
      },
      "id": "U1PL-ATQiT69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the probability distribution for the latent space\n",
        "latent_distribution = tfp.distributions.Normal(loc=0.0, scale=1.0)"
      ],
      "metadata": {
        "id": "vD2sDdH4jK4r"
      },
      "id": "vD2sDdH4jK4r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = x_samples.shape[0] // batch_size\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        start_idx = batch * batch_size\n",
        "        end_idx = (batch + 1) * batch_size\n",
        "        batch_data = x_samples[start_idx:end_idx]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            latent_pred = nice_model.forward(batch_data)\n",
        "\n",
        "            log_prob_forward = latent_distribution.log_prob(latent_pred)\n",
        "            log_det_forward = tf.reduce_sum(tf.math.log(nice_model.diagonal_scale))\n",
        "            log_prob_total = log_prob_forward + log_det_forward\n",
        "\n",
        "            negative_log_likelihood = -tf.reduce_mean(log_prob_total)\n",
        "\n",
        "        grads = tape.gradient(negative_log_likelihood, nice_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, nice_model.trainable_variables))\n",
        "\n",
        "        epoch_loss += negative_log_likelihood.numpy()\n",
        "\n",
        "    print(\"Epoch:\", epoch + 1, \"Loss:\", epoch_loss / num_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBpYnm1XSbNj",
        "outputId": "299aa159-2f53-49ac-dac3-5934df6ddf0b"
      },
      "id": "WBpYnm1XSbNj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f4cd04c3640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f4cd04c3640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 1.2160893762484193\n",
            "Epoch: 2 Loss: 0.9168199375271797\n",
            "Epoch: 3 Loss: 0.8291278276592493\n",
            "Epoch: 4 Loss: 0.7485119551420212\n",
            "Epoch: 5 Loss: 0.6764731155708432\n",
            "Epoch: 6 Loss: 0.6101751327514648\n",
            "Epoch: 7 Loss: 0.5482662892900407\n",
            "Epoch: 8 Loss: 0.48803867492824793\n",
            "Epoch: 9 Loss: 0.4317939789034426\n",
            "Epoch: 10 Loss: 0.3804547644685954\n",
            "Epoch: 11 Loss: 0.33292743377387524\n",
            "Epoch: 12 Loss: 0.28936798335053027\n",
            "Epoch: 13 Loss: 0.25134043261641636\n",
            "Epoch: 14 Loss: 0.21392354421550408\n",
            "Epoch: 15 Loss: 0.17707627156050876\n",
            "Epoch: 16 Loss: 0.1432654428499518\n",
            "Epoch: 17 Loss: 0.11217657514498569\n",
            "Epoch: 18 Loss: 0.08362137543736026\n",
            "Epoch: 19 Loss: 0.05729438195703551\n",
            "Epoch: 20 Loss: 0.03278769088501576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "ILHCzUdTQc6z"
      },
      "id": "ILHCzUdTQc6z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the NICE model with larger hidden layers and Layer Normalization\n",
        "input_dim = 784  # Dimensionality of the input\n",
        "hidden_dim = 500  # Dimensionality of the hidden layer\n",
        "num_coupling_layers = 4  # Number of coupling layers in the mod\n",
        "nice_model = NICEModel(input_dim, hidden_dim, num_coupling_layers)"
      ],
      "metadata": {
        "id": "0GuSL1C0iR5g"
      },
      "id": "0GuSL1C0iR5g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "gPEcgnYUVr9i"
      },
      "id": "gPEcgnYUVr9i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function for maximum likelihood estimation\n",
        "def negative_log_likelihood_loss(y_true, y_pred):\n",
        "    log_prob = latent_distribution.log_prob(y_pred)\n",
        "    return -tf.reduce_mean(log_prob)"
      ],
      "metadata": {
        "id": "axoNt0omRNRz"
      },
      "id": "axoNt0omRNRz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the probability distribution for the latent space\n",
        "latent_distribution = tfp.distributions.Normal(loc=0.0, scale=1.0)"
      ],
      "metadata": {
        "id": "NrCghZ5zYAQ4"
      },
      "id": "NrCghZ5zYAQ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "# steps_per_sample = 100\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = x_train.shape[0] // batch_size\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        start_idx = batch * batch_size\n",
        "        end_idx = (batch + 1) * batch_size\n",
        "        batch_data = x_train[start_idx:end_idx]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            latent_pred = nice_model.forward(batch_data)\n",
        "            loss_value = negative_log_likelihood_loss(batch_data, latent_pred)\n",
        "\n",
        "        grads = tape.gradient(loss_value, nice_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, nice_model.trainable_variables))\n",
        "\n",
        "        epoch_loss += loss_value.numpy()\n",
        "\n",
        "    train_loss = epoch_loss / num_batches\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Calculate validation loss\n",
        "    val_loss = negative_log_likelihood_loss(x_test, nice_model.forward(x_test)).numpy()\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(\"Epoch:\", epoch + 1, \"Train Loss:\", train_loss, \"Val Loss:\", val_loss)\n",
        "\n",
        "    # Generate and plot 8 sample images\n",
        "    # latent_samples = tf.random.normal((10, input_dim))\n",
        "    # generated_images = nice_model.backward(latent_samples)\n",
        "    # generated_images = tf.reshape(generated_images, (10, 28, 28))\n",
        "\n",
        "    # plt.figure(figsize=(10, 4))\n",
        "    # for i in range(8):\n",
        "    #     plt.subplot(2, 4, i + 1)\n",
        "    #     plt.imshow(generated_images[i], cmap='gray')\n",
        "    #     plt.axis('off')\n",
        "    # plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVgEwtSiiXn7",
        "outputId": "2745d07e-0ab3-426d-f056-d38c69f8e752"
      },
      "id": "tVgEwtSiiXn7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Train Loss: 0.9938622418886576 Val Loss: 0.9441293\n",
            "Epoch: 2 Train Loss: 0.9361718764417192 Val Loss: 0.93175805\n",
            "Epoch: 3 Train Loss: 0.9282509213329381 Val Loss: 0.9264776\n",
            "Epoch: 4 Train Loss: 0.9247228475207956 Val Loss: 0.92383116\n",
            "Epoch: 5 Train Loss: 0.9227566166311247 Val Loss: 0.92224157\n",
            "Epoch: 6 Train Loss: 0.9214636952194393 Val Loss: 0.921183\n",
            "Epoch: 7 Train Loss: 0.9205961092415019 Val Loss: 0.9204128\n",
            "Epoch: 8 Train Loss: 0.9200264961801023 Val Loss: 0.91994697\n",
            "Epoch: 9 Train Loss: 0.9196618005760715 Val Loss: 0.91967636\n",
            "Epoch: 10 Train Loss: 0.9194333999075441 Val Loss: 0.9194715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starter"
      ],
      "metadata": {
        "id": "QA47HxoXJ7ia"
      },
      "id": "QA47HxoXJ7ia"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11f3eac",
      "metadata": {
        "id": "d11f3eac"
      },
      "outputs": [],
      "source": [
        "# 1st part: implement coupling layer\n",
        "# - split input\n",
        "# - implement y1 = x1, y2 = x2 + m(y1)   (forward)\n",
        "# - implement backward\n",
        "\n",
        "\n",
        "# one way to split could be\n",
        "x1, x2 = tf.split(x, 2, axis=-1)\n",
        "y1, y2 = whatever_the_coupling_layer_does(x1, x2)\n",
        "y =  tf.concat([y1, y2], axis=-1)\n",
        "\n",
        "# this is not so good, since it splits the data right through the middle.\n",
        "# if the data has spatial relations, like images, this means the model\n",
        "# conditions, say, the upper half of the image on the lower half, and vice versa.\n",
        "# these long-range dependencies are hard to model.\n",
        "\n",
        "# another option could be an even-odd split:\n",
        "def split_even_odd(inp):\n",
        "    even_inds = tf.range(0, inp.shape[1], 2)\n",
        "    odd_inds = tf.range(1, inp.shape[1], 2)\n",
        "\n",
        "    even = tf.gather(inp, even_inds, axis=1)\n",
        "    odd = tf.gather(inp, odd_inds, axis=1)\n",
        "\n",
        "    # process even and odd parts separately...\n",
        "    ...\n",
        "    \n",
        "    # combine\n",
        "    together = tf.stack([even_output, odd_output], axis=-1)\n",
        "    return tf.reshape(together, tf.shape(inp))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1019f718",
      "metadata": {
        "id": "1019f718"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11fc6a4",
      "metadata": {
        "id": "d11fc6a4"
      },
      "outputs": [],
      "source": [
        "# 2nd part: NICE model\n",
        "# - stack a bunch of coupling layers\n",
        "# - switch x1, x2 for each layer\n",
        "# - forward: apply coupling layers\n",
        "# - backward: apply backward coupling layers (in reverse)\n",
        "\n",
        "# - rescaling:\n",
        "# - create d-dimensional vector (model weight)\n",
        "# - forward: multiply x * exp(vector) after applying coupling layers\n",
        "# - backwards: x / vector (or x * tf.exp(-vector)) BEFORE applying reverse coupling layers\n",
        "\n",
        "\n",
        "# - training: map x -> h using NICE model\n",
        "# - compute log_p_simple(h)\n",
        "# - add log determinant of jacobian: simply sum of scaling values\n",
        "# use -log_likelihood as loss\n",
        "\n",
        "# a note on simple distributions.\n",
        "# you should use tfd = tfp.distributions...\n",
        "input_dim = 12  # example\n",
        "batch_size = 8\n",
        "dummy_data = tf.random.normal((batch_size, input_dim))\n",
        "\n",
        "simple_distribution = tfd.Normal(loc=tf.zeros((input_dim,)), scale=tf.ones((input_dim,)))\n",
        "log_p_simple = simple_distribution.log_prob(dummy_data)\n",
        "# ... this will return a batch x dim matrix. then SUM over axis 1 (data dimension)! average over batch axis!\n",
        "log_p_simple = tf.reduce_sum(log_p_simple, axis=1)\n",
        "print(log_p_simple)\n",
        "\n",
        "# alternatively: use multivariate distribution.\n",
        "# this returns one prob per entry.\n",
        "simple_distribution = tfd.MultivariateNormalDiag(loc=tf.zeros((input_dim,)))\n",
        "print(simple_distribution.log_prob(dummy_data))\n",
        "\n",
        "print(\"Same results!\")\n",
        "\n",
        "# just don't mix it up!!! I had a bug where I was using multivariate, but summing over the last dimension.\n",
        "# this resulted in summing my loss over the batch axis instead of averaging.\n",
        "# that's bad, because your effective learning rate is MUCH higher than expected.\n",
        "\n",
        "# the paper proses using tfd.Logistic instead. that does not have a multivariate version AFAIK,\n",
        "# so you will have to use the first option."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, here is a sample toy dataset.\n",
        "# it's a 2D parabola kinda thing.\n",
        "# you can use this to test your flow models.\n",
        "# even a simple model like NICE should be able to fit this!\n",
        "n_samples = 2048\n",
        "x2_dist = tfd.Normal(loc=0., scale=0.5)\n",
        "x2_samples = x2_dist.sample(n_samples)\n",
        "x1 = tfd.Normal(loc=1. * tf.square(x2_samples),\n",
        "                scale=0.1*tf.ones(n_samples, dtype=tf.float32))\n",
        "x1_samples = x1.sample()\n",
        "x_samples = tf.stack([x1_samples, x2_samples], axis=1)\n",
        "\n",
        "as_np = x_samples.numpy()\n",
        "plt.scatter(as_np[:, 0], as_np[:, 1])\n",
        "a = plt.gca()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# x_samples it the dataset!\n",
        "# NOTE that I say in the assignment that LayerNorm > BatchNorm for these models apparently.\n",
        "# but for these simple models with very low data dimensionality, it appears layernorm causes issues sometimes.\n",
        "# so maybe leave normalization out completely."
      ],
      "metadata": {
        "id": "m6p6vZYWItkW"
      },
      "id": "m6p6vZYWItkW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "QA47HxoXJ7ia"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}