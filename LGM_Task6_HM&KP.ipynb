{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "srXC6pLGLwS6",
        "WGyKZj3bzf9p",
        "cKh-4eeDiCfV",
        "o8to8uZ9tw3r",
        "TNt8gg5Zf7FC",
        "_dRrr8pzgLJN",
        "UHjdCjDuSvX_",
        "FeqxGsaQU3bD",
        "LFjSVAlWzf-N"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 6: Autoregressive Language Modeling - Kai Ponel & Hannan Mahadik "
      ],
      "metadata": {
        "id": "3QeHqwBvA0uo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Shakespeare dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_55cOxLkAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "196be1a7-bad2-4b15-fd6a-e6f008fb7900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN (Custom, Bad results)"
      ],
      "metadata": {
        "id": "cKh-4eeDiCfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Create a vocabulary of unique characters in the text\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "# Create a mapping from characters to indices\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "\n",
        "# Create a mapping from indices to characters\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert the text to a sequence of integer indices\n",
        "text_as_int = np.array([char2idx[char] for char in text])\n",
        "\n",
        "# Define the sequence length and create training examples and targets\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "\n",
        "# HParams\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Split DS into train and val\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size).take(val_size)\n",
        "\n",
        "def myModel(vocab_size, embedding_dim, rnn_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim))\n",
        "    model.add(GRU(rnn_units, return_sequences=True, stateful=False,\n",
        "                  kernel_initializer='glorot_uniform'))\n",
        "    model.add(Dense(vocab_size))\n",
        "    return model\n",
        "\n",
        "model = myModel(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "def loss_fn(labels, logits):\n",
        "  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "\n",
        "# Define a function to calculate the loss on the validation set\n",
        "def val_loss(model, val_dataset):\n",
        "    loss = 0\n",
        "    for input_example_batch, target_example_batch in val_dataset:\n",
        "        predictions = model(input_example_batch)\n",
        "        loss += loss_fn(target_example_batch, predictions)\n",
        "    return loss / len(val_dataset)\n",
        "\n",
        "\n",
        "optimizer = tf.optimizers.Adam(clipnorm=1.0)\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    for input_example_batch, target_example_batch in train_dataset:\n",
        "        if np.isnan(input_example_batch.numpy()).any() or np.isnan(target_example_batch.numpy()).any():\n",
        "            print('Data contains nan values')\n",
        "            break\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(input_example_batch)\n",
        "            tf.debugging.check_numerics(predictions, 'predictions contains nan or inf')            \n",
        "            loss = loss_fn(target_example_batch, predictions)\n",
        "            tf.debugging.check_numerics(loss, 'loss contains nan or inf')\n",
        "            epoch_loss_avg.update_state(loss)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    print(f'Epoch {epoch+1}: Train Loss: {epoch_loss_avg.result()}')\n",
        "    print(f'Epoch {epoch+1}: Val Loss: {val_loss(model, val_dataset)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdSm8CXNiGGQ",
        "outputId": "bdef98de-074f-4af9-810d-6de5eccb75ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172\n",
            "Length of train_dataset: 137\n",
            "Starting epoch 1\n",
            "Epoch 1: Train Loss: 2.781325101852417\n",
            "Epoch 1: Val Loss: 2.236217737197876\n",
            "Starting epoch 2\n",
            "Epoch 2: Train Loss: 2.0536627769470215\n",
            "Epoch 2: Val Loss: 1.8738161325454712\n",
            "Starting epoch 3\n",
            "Epoch 3: Train Loss: 1.7691675424575806\n",
            "Epoch 3: Val Loss: 1.6603164672851562\n",
            "Starting epoch 4\n",
            "Epoch 4: Train Loss: 1.5998120307922363\n",
            "Epoch 4: Val Loss: 1.516616940498352\n",
            "Starting epoch 5\n",
            "Epoch 5: Train Loss: 1.4881635904312134\n",
            "Epoch 5: Val Loss: 1.4242022037506104\n",
            "Starting epoch 6\n",
            "Epoch 6: Train Loss: 1.4150744676589966\n",
            "Epoch 6: Val Loss: 1.3632065057754517\n",
            "Starting epoch 7\n",
            "Epoch 7: Train Loss: 1.3578110933303833\n",
            "Epoch 7: Val Loss: 1.302501916885376\n",
            "Starting epoch 8\n",
            "Epoch 8: Train Loss: 1.3105816841125488\n",
            "Epoch 8: Val Loss: 1.2590655088424683\n",
            "Starting epoch 9\n",
            "Epoch 9: Train Loss: 1.2691797018051147\n",
            "Epoch 9: Val Loss: 1.2233033180236816\n",
            "Starting epoch 10\n",
            "Epoch 10: Train Loss: 1.2319068908691406\n",
            "Epoch 10: Val Loss: 1.1808059215545654\n",
            "Starting epoch 11\n",
            "Epoch 11: Train Loss: 1.1942628622055054\n",
            "Epoch 11: Val Loss: 1.1420012712478638\n",
            "Starting epoch 12\n",
            "Epoch 12: Train Loss: 1.1583540439605713\n",
            "Epoch 12: Val Loss: 1.1010942459106445\n",
            "Starting epoch 13\n",
            "Epoch 13: Train Loss: 1.119134783744812\n",
            "Epoch 13: Val Loss: 1.0565115213394165\n",
            "Starting epoch 14\n",
            "Epoch 14: Train Loss: 1.0811816453933716\n",
            "Epoch 14: Val Loss: 1.0084729194641113\n",
            "Starting epoch 15\n",
            "Epoch 15: Train Loss: 1.041834831237793\n",
            "Epoch 15: Val Loss: 0.9737295508384705\n",
            "Starting epoch 16\n",
            "Epoch 16: Train Loss: 1.0027786493301392\n",
            "Epoch 16: Val Loss: 0.9323287606239319\n",
            "Starting epoch 17\n",
            "Epoch 17: Train Loss: 0.9588775634765625\n",
            "Epoch 17: Val Loss: 0.8819432854652405\n",
            "Starting epoch 18\n",
            "Epoch 18: Train Loss: 0.9172601103782654\n",
            "Epoch 18: Val Loss: 0.8412991762161255\n",
            "Starting epoch 19\n",
            "Epoch 19: Train Loss: 0.8737011551856995\n",
            "Epoch 19: Val Loss: 0.7994124889373779\n",
            "Starting epoch 20\n",
            "Epoch 20: Train Loss: 0.8342109322547913\n",
            "Epoch 20: Val Loss: 0.7572619318962097\n",
            "Starting epoch 21\n",
            "Epoch 21: Train Loss: 0.7961140275001526\n",
            "Epoch 21: Val Loss: 0.7239345908164978\n",
            "Starting epoch 22\n",
            "Epoch 22: Train Loss: 0.7584589719772339\n",
            "Epoch 22: Val Loss: 0.6820416450500488\n",
            "Starting epoch 23\n",
            "Epoch 23: Train Loss: 0.7236728072166443\n",
            "Epoch 23: Val Loss: 0.6500006318092346\n",
            "Starting epoch 24\n",
            "Epoch 24: Train Loss: 0.691108226776123\n",
            "Epoch 24: Val Loss: 0.6258907318115234\n",
            "Starting epoch 25\n",
            "Epoch 25: Train Loss: 0.664669930934906\n",
            "Epoch 25: Val Loss: 0.6032058000564575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string=None, num_generate_chars=1000, temperature=1.0):\n",
        "    # Convert the start string to a sequence of integer indices\n",
        "    if start_string:\n",
        "        input_eval = [char2idx[s] for s in start_string]\n",
        "        input_eval = tf.expand_dims(input_eval, 0)\n",
        "    else:\n",
        "        input_eval = tf.expand_dims([], 0)\n",
        "\n",
        "    generated_text = []\n",
        "\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(num_generate_chars):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        generated_text.append(idx2char[predicted_id])\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return ''.join(generated_text)\n",
        "print(generate_text(model, start_string=\"C\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7JzbgeiKAf",
        "outputId": "19cfaf2f-3713-483c-a2c5-66728f0df4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LORO:\n",
            "Welfickin aichteelengeane me,\n",
            "\n",
            "\n",
            "\n",
            "RUCLo m, atite be dyousirthirachay whees m qus\n",
            "TI are t'ser allecefoukisompe toutherdurngenonee, la the thond tsean GHe penghef hert balld t a. wh tand, therd ge tonof wh ber t:\n",
            "\n",
            "METINThande youne witorit gath bur ll g lllds, me cad hind ard it juthatho ileneramug t tist mowan mallathigor d akind toknanopl y ul s y y inofo t, and ts ththes onguret.\n",
            "S:\n",
            "\n",
            "L nco:\n",
            "\n",
            "IS &CHAMINThast totors an IXESe meng mear! t be wil githe sit hit notin ak's bl od:\n",
            "\n",
            "\n",
            "\n",
            "ETHoune thithor oo aseldshelu tond pl chifesJO\n",
            "HE:\n",
            "ANLanof f blit y s ckealinstwansther t y tevinoo; yo conchis fin a Mixe atitur I t an\n",
            "I ste thanchon:\n",
            "S:\n",
            "O:\n",
            "\n",
            "Th ce ou al cif cellld h thonger s thetheave\n",
            "\n",
            "Cowhet hatham t wnol henopof tie\n",
            "ANGof aterds,\n",
            "XI atit IO:\n",
            "F h beere acace, hon ere ne;\n",
            "Hore waved haveardomacerd n al t wet t paco tontonamous I akize wnon h, be myo le t OUSCAMID:\n",
            "Theyeldursicacknd-berebld har s asthe, d n,\n",
            "O mat e; ND maral, malo nge y a thobey tow frd heve tinghe.\n",
            "NUpan his hires hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial based"
      ],
      "metadata": {
        "id": "o8to8uZ9tw3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rrHfeV9LYIK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, GRU, Dropout, Input\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "9eUetO3dL4qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vaUZ0bEL2mI",
        "outputId": "aa589a96-3b3c-4098-dec2-1120af4879e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2v8NSANL79G",
        "outputId": "e6132bd3-c120-458b-e0a6-947604943a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))"
      ],
      "metadata": {
        "id": "iVGUrdovMY5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "6mTBAyGKMpNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzFTe1JwMlP8",
        "outputId": "50ffd1ed-9a65-41fa-9e7d-37d2494d3798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "bMkYVykmNYYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))"
      ],
      "metadata": {
        "id": "8ndsUg0uNbXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "tuvViYZUNhF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 10000\n",
        "seq_length = 50"
      ],
      "metadata": {
        "id": "bZAj9lt6NenW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "MTq_SD1FSMN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "6pjKy7neL9Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "NMEdQyYxP0fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQZmjzeXSjov",
        "outputId": "e50dfc53-b442-44ae-ad0d-46540babeb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AwYGjwkQWZD",
        "outputId": "690f246f-4af7-4b28-c55a-8ec56a240a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(128, 50), dtype=tf.int64, name=None), TensorSpec(shape=(128, 50), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "sar7hriiSvXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "oPmatmBoPUkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "otj2SvCiQkgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "S3IQJL42PgT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "J42s-oAAPj4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FxqfaX5Pooe",
        "outputId": "306c7faf-6186-41f7-9bb7-9c0986330c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "170/170 [==============================] - 12s 52ms/step - loss: 2.7409\n",
            "Epoch 2/10\n",
            "170/170 [==============================] - 9s 48ms/step - loss: 2.0072\n",
            "Epoch 3/10\n",
            "170/170 [==============================] - 10s 48ms/step - loss: 1.7371\n",
            "Epoch 4/10\n",
            "170/170 [==============================] - 9s 50ms/step - loss: 1.5797\n",
            "Epoch 5/10\n",
            "170/170 [==============================] - 9s 50ms/step - loss: 1.4812\n",
            "Epoch 6/10\n",
            "170/170 [==============================] - 9s 49ms/step - loss: 1.4138\n",
            "Epoch 7/10\n",
            "170/170 [==============================] - 9s 50ms/step - loss: 1.3611\n",
            "Epoch 8/10\n",
            "170/170 [==============================] - 9s 50ms/step - loss: 1.3153\n",
            "Epoch 9/10\n",
            "170/170 [==============================] - 9s 50ms/step - loss: 1.2730\n",
            "Epoch 10/10\n",
            "170/170 [==============================] - 10s 50ms/step - loss: 1.2314\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4cbba58250>"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "w_lZgHIVRBfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "dDvR2iUBREOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "# end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "# print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqBATfxpRIZX",
        "outputId": "f910b9b4-1508-4e99-cd98-b45d68b10d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "That shall be julk, before:\n",
            "The general is put the thunder of nobody.\n",
            "\n",
            "SICINIUS:\n",
            "Nay, then I look'd oath, to fine ourselves, son and have no such face\n",
            "The strange many nuts bite us for two'ers to wounded?\n",
            "\n",
            "JULIET:\n",
            "O, falling look me, and I am Duke of Nare:\n",
            "He let us sleep of speed;\n",
            "When virties you stard flaw'd, and each of me and loot former,\n",
            "I never said it fellest to the poxuls,\n",
            "For now no succes it in my leave; and then\n",
            "I grieve yield enjury thee on that, in their presence, and be known bey,\n",
            "An I bigg'd the days of aws,\n",
            "His scaptal flay woe well we'll Bianca say 'callo?\n",
            "Dispatch she is dead.\n",
            "\n",
            "First Gentleman:\n",
            "When you thank you, sir, for I can are in my\n",
            "fair; and, to rather thanksope,\n",
            "Where did new run you shall. Pray you a lad, have supposeth\n",
            "The pernice that mighonces\n",
            "Which oft dear soul of that witness.\n",
            "\n",
            "LUCENTIO:\n",
            "Claudio, this depose and few,\n",
            "That madam: but longest truch me 's't a dishost.\n",
            "Our decree dishonour, I say.\n",
            "\n",
            "GREMIO:\n",
            "They came, I with a joy on stroke;--\n",
            "That I may b \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer (BadGPT)\n"
      ],
      "metadata": {
        "id": "TNt8gg5Zf7FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Masking, Dropout, LSTM, Attention\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load the Shakespeare dataset\n",
        "shakespeare_path = tf.keras.utils.get_file(\n",
        "    'shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        ")\n",
        "text = open(shakespeare_path, 'rb').read().decode(encoding='utf-8')\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Preprocess the text\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Create input and target sequences\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "GBvr_nPChL-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "embedding_dim = 512\n",
        "rnn_units = 2048\n",
        "batch_size = 256\n",
        "\n",
        "# Create the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    inputs = Input(shape=(None,))\n",
        "    x = Embedding(vocab_size, embedding_dim)(inputs)\n",
        "    x = Masking(mask_value=0.0)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = LSTM(rnn_units, return_sequences=True)(x)\n",
        "    x = Attention()([x, x])\n",
        "    x = Dense(vocab_size)(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "model = build_model(len(vocab), embedding_dim, rnn_units, batch_size)\n",
        "\n",
        "# Prepare training data\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfL8hm9RcbQ6",
        "outputId": "5545b37c-87e5-4eeb-c763-05af22fb36d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "43/43 [==============================] - 48s 1s/step - loss: 3.5802\n",
            "Epoch 2/10\n",
            "43/43 [==============================] - 45s 997ms/step - loss: 3.2153\n",
            "Epoch 3/10\n",
            "43/43 [==============================] - 45s 1s/step - loss: 2.8229\n",
            "Epoch 4/10\n",
            "43/43 [==============================] - 44s 983ms/step - loss: 1.8668\n",
            "Epoch 5/10\n",
            "43/43 [==============================] - 45s 1s/step - loss: 1.0493\n",
            "Epoch 6/10\n",
            "43/43 [==============================] - 44s 994ms/step - loss: 0.8439\n",
            "Epoch 7/10\n",
            "43/43 [==============================] - 45s 1s/step - loss: 0.7292\n",
            "Epoch 8/10\n",
            "43/43 [==============================] - 45s 993ms/step - loss: 0.6772\n",
            "Epoch 9/10\n",
            "43/43 [==============================] - 45s 1s/step - loss: 0.6778\n",
            "Epoch 10/10\n",
            "43/43 [==============================] - 44s 986ms/step - loss: 0.5506\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f631d672b90>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the trained model\n",
        "def generate_text(model, start_string):\n",
        "    num_generate = 1000\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = pad_sequences([input_eval], maxlen=seq_length)\n",
        "    text_generated = []\n",
        "    temperature = 1.0\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model.predict(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = pad_sequences([[predicted_id]], maxlen=seq_length)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Generate text with a starting prompt\n",
        "generated_text = generate_text(model, start_string=\"ROMEO: \")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "JAc7wg6uhJyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial / Previous stuff (Ignore)"
      ],
      "metadata": {
        "id": "_dRrr8pzgLJN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ac165e-a1c4-48db-eb42-5a597215a40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7502d3b-372b-40b6-85f9-958c8b1081ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a504106c-6f85-4403-b43f-6c2d8fc885b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "FeqxGsaQU3bD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation. \n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515dd74b-7d93-4375-c2f7-c5f48b1dabf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9563227-d256-4179-ecc0-eb538b30ecbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1c2b5b-1e9e-49da-9767-cfb31ddf194c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0036e77-4170-40fe-f854-86c8790830ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))"
      ],
      "metadata": {
        "id": "rXNosJASW3ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "Ho1lpRP5W4hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "buffer_size = 10000\n",
        "seq_length = 200"
      ],
      "metadata": {
        "id": "TNlNRmuyXEJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "PLXgVzGQXBmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "x6t-aHOXYDfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "rUv1xWMoYMUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.shuffle(buffer_size).batch(batch_size)"
      ],
      "metadata": {
        "id": "pJ8svzJdYeA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "O3tMcu55YoBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myModel(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_units, return_sequences=True, stateful=True))\n",
        "    model.add(Dense(vocab_size))\n",
        "    return model"
      ],
      "metadata": {
        "id": "sL9JmH7TY0jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = myModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=batch_size)"
      ],
      "metadata": {
        "id": "zv14oA5BZMY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(labels, logits):\n",
        "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)"
      ],
      "metadata": {
        "id": "HjAce8c8Zgri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.optimizers.Adam()"
      ],
      "metadata": {
        "id": "sNknK2KfZ59b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  "
      ],
      "metadata": {
        "id": "IrPaMRjxdefa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "      print('Start of epoch', epoch)\n",
        "\n",
        "      for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n",
        "\n",
        "        skip_ids = ids_from_chars(['[UNK]'])[:, None]\n",
        "\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "\n",
        "        prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "        input_chars = chars_from_ids(x_batch_train)\n",
        "        input_chars = tf.strings.unicode_split(input_chars, 'UTF-8')\n",
        "\n",
        "        # mask = tf.sequence_mask(x_batch_train,dtype=tf.float32,maxlen=1)\n",
        "        mask = tf.reshape(prediction_mask, shape=[batch_size, tf.shape(x_batch_train)[1]])\n",
        "\n",
        "        # input_ids = ids_from_chars(x_batch_train).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            logits = model(x_batch_train) \n",
        "\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "            masked_loss = tf.math.multiply(loss_value, prediction_mask)\n",
        "\n",
        "        grads = tape.gradient(masked_loss, model.trainable_weights)\n",
        "\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "      "
      ],
      "metadata": {
        "id": "VYRuKU07aORP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}